{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84ff307-1f5b-4341-9bd9-90d82b9a60b1",
   "metadata": {},
   "source": [
    "# H - Header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2fd9fc-433c-48fe-a986-f74fb1c692e4",
   "metadata": {},
   "source": [
    "#### H1 - Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2893cbc5-099e-469b-88d6-3481d734115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard foundational libraries\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## import specific functions\n",
    "from os                 import mkdir, listdir\n",
    "from os.path            import isfile, isdir\n",
    "from datetime           import datetime, timedelta\n",
    "from cartopy            import feature\n",
    "from cartopy.crs        import LambertConformal, PlateCarree\n",
    "from dbfread            import DBF\n",
    "from docx               import Document\n",
    "from textwrap           import fill as txt_wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e46a131-c865-43fc-80be-1c2c1576466f",
   "metadata": {},
   "source": [
    "#### H2 - Basic Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e061a3d-8cb6-4bd5-8f67-881c337b1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up standard directories if needed\n",
    "def make_standard_file_system():\n",
    "    for i in ['A_Input', 'B_Intermediate', 'C_Output']:\n",
    "        if not isdir(i): mkdir(i)\n",
    "\n",
    "## log time elapsed\n",
    "time_log = dict()\n",
    "def log_time(the_id = 'End Log'):\n",
    "    \n",
    "    ## construct new time stamp\n",
    "    now_time = str(datetime.now().hour).zfill(2)\n",
    "    now_time = now_time +':'+ str(datetime.now().minute).zfill(2)\n",
    "    now_time = now_time +':'+ str(datetime.now().second).zfill(2)\n",
    "\n",
    "    ## add to time log\n",
    "    if the_id == 'End Log':\n",
    "        time_log['End'] = now_time\n",
    "        print('Time log:')\n",
    "        for i in time_log.keys():\n",
    "            print(i.rjust(5) + ':', time_log[i])\n",
    "    else:\n",
    "        time_log[the_id] = now_time\n",
    "        \n",
    "## toggle cache versus build\n",
    "def build_or_cache(function, address, permit):\n",
    "    if permit and isfile(address):\n",
    "        print('Build/Cache Decision: Cache')\n",
    "        the_file = pd.read_csv(address, index_col = 0)\n",
    "    else:\n",
    "        print('Build/Cache Decision: Build')\n",
    "        the_file = function()\n",
    "    return the_file\n",
    "    \n",
    "## execute functions\n",
    "make_standard_file_system()\n",
    "log_time('H2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12f165-5257-40f4-b625-5950dac2d955",
   "metadata": {},
   "source": [
    "#### H3 - Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9577a3a0-6eb6-4a48-a510-a8d779269232",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set color palette\n",
    "set_color = {\n",
    "    'AzureDark'    :(7/12, 1.0, 0.4),\n",
    "    'AzureMedium'  :(7/12, 0.7, 0.7),\n",
    "    'AzureLight'   :(7/12, 0.4, 1.0),\n",
    "    'AzureBG'      :(7/12, 0.1, 1.0),\n",
    "    'AzureOverlay' :(7/12, 0.1, 1.0, 0.5),\n",
    "    \n",
    "    'OrangeDark'   :(1/12, 1.0, 0.4),\n",
    "    'OrangeMedium' :(1/12, 0.7, 0.7),\n",
    "    'OrangeLight'  :(1/12, 0.4, 1.0),\n",
    "    'OrangeBG'     :(1/12, 0.1, 1.0),\n",
    "    'OrangeOverlay':(1/12, 0.1, 1.0, 0.5) \n",
    "    }\n",
    "\n",
    "## set font sizes\n",
    "set_font = {\n",
    "    'small' : 16,\n",
    "    'medium': 24,\n",
    "    'large' : 32\n",
    "    }\n",
    "\n",
    "## time-saver settings\n",
    "set_acceleration = {\n",
    "    'dbf_cache':True,\n",
    "    'sample_size':1/20,\n",
    "    'distance_cache':False,\n",
    "    'cluster_cache':False\n",
    "    }\n",
    "\n",
    "## map parameters\n",
    "set_map = {\n",
    "    'bounds'  : [-124.73 + 5, -66.95 - 5, 25.12 - 3.3, 49.38 + 3.3],\n",
    "    'map_proj': LambertConformal(\n",
    "        central_longitude = (-124.73 - 66.95) / 2,\n",
    "        central_latitude = (25.12 + 49.38) / 2,\n",
    "        standard_parallels = (25.12, 49.38)\n",
    "        )\n",
    "    }\n",
    "\n",
    "log_time('H3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255532f-66e3-45f4-b62c-aaf8f2941744",
   "metadata": {},
   "source": [
    "# GD - Gather Data / RD - Refine Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149febf-9577-46d1-80f9-aa0a37dad8f9",
   "metadata": {},
   "source": [
    "#### GD1 - read census tract geographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b030f7-3b12-41e6-9dc5-1fe7eabbf20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build/Cache Decision: Cache\n"
     ]
    }
   ],
   "source": [
    "def read_geo_data(directory = 'A_Input/tracts_dbf'):\n",
    "    \n",
    "    ## list dbf files in target directory\n",
    "    dbf_addr = listdir(directory)\n",
    "    dbf_addr = [i for i in dbf_addr if i[-3::] == 'dbf']\n",
    "    \n",
    "    ## define relevant columns from each\n",
    "    desired_columns = {'GEOID': str,\n",
    "                       'STATEFP': str, 'COUNTYFP':str, 'TRACTCE':str,\n",
    "                        'INTPTLAT': float, 'INTPTLON': float, 'ALAND': int}\n",
    "    \n",
    "    ## read in all dbf files\n",
    "    dbf_data = []\n",
    "    for i in dbf_addr:\n",
    "        i_dbf = pd.DataFrame(iter(DBF(directory + '/' + i)))\n",
    "        i_dbf = i_dbf[desired_columns.keys()].astype(desired_columns)\n",
    "        dbf_data.append(i_dbf)\n",
    "        \n",
    "    ## compile data into a single file\n",
    "    dbf_data = pd.concat(dbf_data, axis = 0).sort_values('GEOID')\n",
    "    dbf_data['count'] = 1\n",
    "    dbf_data = dbf_data.reset_index(drop = True)\n",
    "    \n",
    "    ## export data\n",
    "    dbf_data.to_csv('B_Intermediate/dbf_data.csv.gz')\n",
    "    return dbf_data\n",
    "\n",
    "## execute code\n",
    "geo_data = build_or_cache(function = read_geo_data,\n",
    "                          address = 'B_Intermediate/dbf_data.csv.gz',\n",
    "                          permit = set_acceleration['dbf_cache'])\n",
    "log_time('GD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b44dd8-b3a4-4eff-a644-21bb455d388a",
   "metadata": {},
   "source": [
    "#### RD1 - draw a sample from the census tract geographic data and exclude outlier tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3fd248c-6dab-40fe-83a5-2228ac349e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_geo_data(dat = geo_data, too_rural = 20.720e6 * 5,\n",
    "                    too_much = set_acceleration['sample_size']):\n",
    "    \n",
    "    ## filter out extremely rural areas (< 100 people per square mile)\n",
    "    dat = dat.loc[dat.ALAND < too_rural, :]\n",
    "    \n",
    "    ## take systematic sample of the data\n",
    "    i = np.arange(0, dat.shape[0]) % int(1 / too_much)\n",
    "    dat = dat.loc[i == 0, ]\n",
    "\n",
    "    ## return data\n",
    "    return dat\n",
    "\n",
    "## execute code\n",
    "geo_data = refine_geo_data()\n",
    "log_time('RD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3373db0-4b46-4aac-8b01-9e6d8dfd92da",
   "metadata": {},
   "source": [
    "#### GD2 - read population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28942f36-b9af-41e1-a844-c489d8d20c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pop_data(roster = 'A_Input/sources.csv'):\n",
    "    \n",
    "    ## read in data file roster\n",
    "    roster = pd.read_csv(roster).set_index('OBJ_NAME')\n",
    "    \n",
    "    ## load all datasets in the roster file\n",
    "    pop_data = dict()\n",
    "    for i in roster.index:\n",
    "        var_names = roster.loc[i, 'VAR_NAME'].split(';')\n",
    "        pop_data[i] = pd.read_csv('A_Input' + '/' + roster.loc[i, 'FILE_NAME'],\n",
    "            usecols = var_names, dtype = str)\n",
    "        \n",
    "    ## merge census datasets\n",
    "    census_i = roster.index[roster.SOURCE == 'data.census.gov'].values\n",
    "    census_dat_count = 0\n",
    "    \n",
    "    for i in census_i:\n",
    "        pop_data[i] = pop_data[i].loc[1::, :].set_index('GEO_ID')\n",
    "        if census_dat_count < 1:\n",
    "            pop_data['census'] = pop_data[i]\n",
    "            pop_data.pop(i)\n",
    "            census_dat_count += 1\n",
    "        else:\n",
    "            pop_data['census'] = pop_data['census'].join(pop_data[i])\n",
    "            pop_data.pop(i)\n",
    "            census_dat_count += 1\n",
    "            \n",
    "    ## convert census data to numeric\n",
    "    def robust_int(x):\n",
    "        try: x = int(x)\n",
    "        except: x = 0\n",
    "        return x\n",
    "    map_robust_int = lambda x: x.map(robust_int)\n",
    "    pop_data['census'] = pop_data['census'].apply(map_robust_int)\n",
    "\n",
    "    return pop_data\n",
    "\n",
    "## execute code\n",
    "pop_data = read_pop_data()\n",
    "log_time('GD2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd744f8f-2948-4bc1-b8f3-61a41011a660",
   "metadata": {},
   "source": [
    "#### RD2 - refine and compile population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ba287d1-300b-42d3-a930-6865bcfab541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_pop_data(pop):\n",
    "    \n",
    "    ## -- standardize geographic codes as needed\n",
    "    state_fips = pop.pop('state_fips')# if needed in future; not currently used\n",
    "    pop['census'].index = pop['census'].index.str.replace('1400000US', '')\n",
    "\n",
    "    ## -- refine life expectancy data and merge into census data\n",
    "    pop['lifespan'].columns = ['GEO_ID', 'life_expect']\n",
    "    pop['lifespan'].life_expect = pop['lifespan'].life_expect.astype(float)\n",
    "    pop['census'] = pop['census'].join(pop['lifespan'].set_index('GEO_ID'))\n",
    "    pop.pop('lifespan')\n",
    "    \n",
    "    ## -- refine voting data\n",
    "    \n",
    "    ## filter to necessary data\n",
    "    i = (pop['vote'].party == 'REPUBLICAN') & (pop['vote'].year == '2020')\n",
    "    pop['vote'] = pop['vote'].loc[i].drop(['year', 'party', 'state_po'], axis = 1)\n",
    "    \n",
    "    ## impute total votes (data is irregular from state to state)\n",
    "    temp = pop['vote'].copy()\n",
    "    temp = temp.drop(['totalvotes'], axis = 1)\n",
    "    temp = temp.set_index(['county_fips', 'mode']).astype(int).reset_index()\n",
    "    temp = temp.groupby(['county_fips', 'mode']).sum().reset_index()\n",
    "    total_vote = temp.loc[temp['mode'] == 'TOTAL'].set_index('county_fips')\n",
    "    total_vote = total_vote.drop('mode', axis = 1)\n",
    "    seg_vote = temp.loc[temp['mode'] != 'TOTAL'].groupby('county_fips').sum()\n",
    "    total_vote = pd.concat({'Total': total_vote, 'Alt': seg_vote}, axis = 1)\n",
    "    total_vote = total_vote.max(axis = 1)\n",
    "    total_vote = pd.DataFrame({'repvotes':total_vote})\n",
    "    pop['vote'] = pop['vote'].join(total_vote, on = 'county_fips')\n",
    "    pop['vote'] = pop['vote'].drop_duplicates('county_fips')\n",
    "    pop['vote'] = pop['vote'].drop(['mode', 'candidatevotes'], axis = 1)\n",
    "    del seg_vote, temp, total_vote\n",
    "    \n",
    "    ## calculate percentage voting republican\n",
    "    pop['vote'] = pop['vote'].set_index('county_fips').astype(float)\n",
    "    pop['vote']['reppct'] = pop['vote']['repvotes'] / pop['vote']['totalvotes']\n",
    "    \n",
    "    ## calculate percentage of population that voted\n",
    "    county_total = pd.DataFrame(pop['census']['DP05_0001E'])\n",
    "    county_total['county'] = [i[0:5] for i in county_total.index]\n",
    "    county_total = county_total.groupby('county').sum().astype(int)\n",
    "    pop['vote'] = pop['vote'].join(county_total)\n",
    "    pop['vote']['totalpct'] = pop['vote']['totalvotes'] / pop['vote']['DP05_0001E']\n",
    "    pop['vote'].loc[pop['vote'].totalpct > 1, 'totalpct'] = 159633396 / 331449281\n",
    "    pop['vote'] = pop['vote'][['reppct', 'totalpct']]\n",
    "    \n",
    "    ## merge voting data into census and convert to counts\n",
    "    pop['census']['county'] = [i[0:5] for i in pop['census'].index]\n",
    "    pop['census'] = pop['census'].reset_index().set_index('county').join(pop['vote'])\n",
    "    pop = pop['census'].set_index('GEO_ID')\n",
    "    pop['state'] = [i[0:2] for i in pop.index]\n",
    "    \n",
    "    ## -- impute missing data\n",
    "    \n",
    "    ## impute at the state level\n",
    "    state_mean = pop.copy()[['life_expect', 'reppct', 'totalpct', 'state']]\n",
    "    state_mean = state_mean.groupby('state').mean().round(2)\n",
    "    state_mean = state_mean.loc[pop.state]\n",
    "    for i in state_mean.columns:\n",
    "        j = pop[i].isna().values\n",
    "        pop.loc[j, i] = state_mean.loc[j, i].values\n",
    "    \n",
    "    ## impute at the national level\n",
    "    for i in state_mean.columns:\n",
    "        j = pop[i].isna().values\n",
    "        pop.loc[j, i] = pop[i].mean()\n",
    "    del state_mean\n",
    "    \n",
    "    ## convert vote proportions to counts\n",
    "    pop['rep_vote'] = pop['reppct'] * pop['totalpct'] * pop['DP05_0001E']\n",
    "    pop['rep_vote'] = pop['rep_vote'].round().astype(int)\n",
    "    pop['total_vote'] = (pop['totalpct'] * pop['DP05_0001E']).round().astype(int)\n",
    "    pop = pop.drop(['reppct', 'totalpct', 'state'], axis = 1).round(1)\n",
    "    \n",
    "    return pop\n",
    "\n",
    "## execute code\n",
    "pop_data = refine_pop_data(pop = pop_data.copy())\n",
    "log_time('RD2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc48382-fdbf-47c5-b7d1-0cbf51cc5691",
   "metadata": {},
   "source": [
    "#### GD3 / RD3 - read and refine text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1a2da6a-4fc5-472a-b366-373019afd73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_explanatory_text(addr = 'A_Input/explanation.docx', n = 47):\n",
    "    explain = Document(addr).paragraphs\n",
    "    explain = [txt_wrap(i.text, n) for i in explain]\n",
    "    explain = '\\n'.join(explain)\n",
    "    return explain\n",
    "\n",
    "## execute code\n",
    "explanatory_text = read_explanatory_text()\n",
    "log_time('RD3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df65672-a010-40b2-94ac-3906eae80072",
   "metadata": {},
   "source": [
    "# Model Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960839f-ac9e-4197-bac4-d6dd25eab70d",
   "metadata": {},
   "source": [
    "#### MD1 - Reconcile geographic and population dataset tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470b8b1-5eb8-4a8b-ae8e-73f34d99cc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90542624-5a86-4e97-b5a6-1d3c4845dd75",
   "metadata": {},
   "source": [
    "#### MD2 - Precalculate tract-to-tract distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb3cca-b253-4240-8d2f-15f99fa62a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc19f1f2-f863-4872-8427-023b5a662713",
   "metadata": {},
   "source": [
    "#### MD3 - Calculate helper functions for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a3688-72ee-4f26-8487-ec47ad835e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ceac255-b803-4fd3-a016-63a5473ef61b",
   "metadata": {},
   "source": [
    "#### MD4 - Conduct two-stage agglomeration clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f5eba1-fa93-4217-9df6-626a48df66d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1665fce9-de4f-4319-923f-e351217c78a3",
   "metadata": {},
   "source": [
    "# Enrich Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85238579-7718-4801-bda4-a45d6d7d5485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720fc5b-7dd3-40ae-ba79-a0bfd874e3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d58595a4-ffa6-407a-9fce-a86f9bfadef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time log:\n",
      "   H2: 13:20:51\n",
      "   H3: 13:20:51\n",
      "  GD1: 13:20:51\n",
      "  RD1: 13:20:51\n",
      "  GD2: 13:20:57\n",
      "  RD2: 13:20:57\n",
      "  RD3: 13:20:57\n",
      "  End: 13:20:57\n"
     ]
    }
   ],
   "source": [
    "log_time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
