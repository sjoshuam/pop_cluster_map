{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54ca8b13-5c8a-41df-b762-d948d0e36e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84ff307-1f5b-4341-9bd9-90d82b9a60b1",
   "metadata": {},
   "source": [
    "# H - Header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2fd9fc-433c-48fe-a986-f74fb1c692e4",
   "metadata": {},
   "source": [
    "#### H1 - Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2893cbc5-099e-469b-88d6-3481d734115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard foundational libraries\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## import specific functions\n",
    "from matplotlib.colors  import hsv_to_rgb, to_hex\n",
    "from os                 import mkdir, listdir\n",
    "from os.path            import isfile, isdir\n",
    "from datetime           import datetime, timedelta\n",
    "from cartopy            import feature\n",
    "from cartopy.crs        import LambertConformal, PlateCarree\n",
    "from dbfread            import DBF\n",
    "from docx               import Document\n",
    "from textwrap           import fill as txt_wrap\n",
    "from geopy.distance     import geodesic\n",
    "from ipyparallel        import Cluster\n",
    "from sklearn.cluster    import AgglomerativeClustering\n",
    "from functools          import partial\n",
    "\n",
    "from scipy.spatial      import Voronoi\n",
    "from shapely.geometry   import Polygon\n",
    "from shapely.ops        import unary_union\n",
    "from shapely.validation import make_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e46a131-c865-43fc-80be-1c2c1576466f",
   "metadata": {},
   "source": [
    "#### H2 - Basic Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e061a3d-8cb6-4bd5-8f67-881c337b1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up standard directories if needed\n",
    "def make_standard_file_system():\n",
    "    for i in ['A_Input', 'B_Intermediate', 'C_Output']:\n",
    "        if not isdir(i): mkdir(i)\n",
    "\n",
    "## log time elapsed\n",
    "time_log = dict()\n",
    "def log_time(the_id = 'End Log'):\n",
    "    \n",
    "    ## construct new time stamp\n",
    "    now_time = str(datetime.now().hour).zfill(2)\n",
    "    now_time = now_time +':'+ str(datetime.now().minute).zfill(2)\n",
    "    now_time = now_time +':'+ str(datetime.now().second).zfill(2)\n",
    "\n",
    "    ## add to time log\n",
    "    if the_id == 'End Log':\n",
    "        time_log['End'] = now_time\n",
    "        print('Time log:')\n",
    "        for i in time_log.keys():\n",
    "            print(i.rjust(5) + ':', time_log[i])\n",
    "    else:\n",
    "        time_log[the_id] = now_time\n",
    "        \n",
    "## toggle cache versus build\n",
    "def build_or_cache(function, address, permit):\n",
    "    if permit and isfile(address):\n",
    "        print('Build/Cache Decision: Cache')\n",
    "        the_file = pd.read_csv(address, index_col = 0)\n",
    "    else:\n",
    "        print('Build/Cache Decision: Build')\n",
    "        the_file = function()\n",
    "        the_file.to_csv(address)\n",
    "    return the_file\n",
    "    \n",
    "## execute functions\n",
    "make_standard_file_system()\n",
    "log_time('H2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12f165-5257-40f4-b625-5950dac2d955",
   "metadata": {},
   "source": [
    "#### H3 - Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9577a3a0-6eb6-4a48-a510-a8d779269232",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set color palette\n",
    "set_color = {\n",
    "    'AzureDark'    :(7/12, 1.0, 0.4),\n",
    "    'AzureMedium'  :(7/12, 0.7, 0.7),\n",
    "    'AzureLight'   :(7/12, 0.4, 1.0),\n",
    "    'AzureBG'      :(7/12, 0.1, 1.0),\n",
    "    \n",
    "    'OrangeDark'   :(1/12, 1.0, 0.4),\n",
    "    'OrangeMedium' :(1/12, 0.7, 0.7),\n",
    "    'OrangeLight'  :(1/12, 0.4, 1.0),\n",
    "    'OrangeBG'     :(1/12, 0.1, 1.0)\n",
    "    }\n",
    "for i in set_color.keys(): set_color[i] = to_hex(hsv_to_rgb(set_color[i]))\n",
    "\n",
    "## set font sizes\n",
    "set_font = {\n",
    "    'small' : 14,\n",
    "    'medium': 21,\n",
    "    'large' : 28\n",
    "    }\n",
    "\n",
    "## time-saver settings\n",
    "set_acceleration = {\n",
    "    'dbf_cache'              : True, #1:40\n",
    "    'sample_size'            : 1/10, #1/5, 1/10, 1/20 = 60, 19, 06 runtime minutes\n",
    "    'pop_cache'              : True,\n",
    "    'distance_parallel_cores': 8,\n",
    "    'distance_cache'         : True,\n",
    "    'cluster_parallel_cores' : 8,\n",
    "    'cluster_L1_cache'       : True,\n",
    "    'cluster_L2_cache'       : True\n",
    "    }\n",
    "\n",
    "## map parameters\n",
    "set_map = {\n",
    "    'bounds'  : [-124.73 + 5, -66.95 - 5, 25.12 - 3.3, 49.38 + 3.3],\n",
    "    'map_proj': LambertConformal(\n",
    "        central_longitude = (-124.73 - 66.95) / 2,\n",
    "        central_latitude = (25.12 + 49.38) / 2,\n",
    "        standard_parallels = (25.12, 49.38))\n",
    "    }\n",
    "log_time('H3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048528d8-d27b-4549-bbcf-22c65d8d2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- variable codes codes\n",
    "## DP02_0068E - Bachlor's Degree* (out of total pop)\n",
    "## DP03_0088E - Income Per Capita** (compare to average)\n",
    "## DP05_0001E - Total Population*\n",
    "## DP05_0077E - Non-Hispanic White\n",
    "## life_expect - life expectancy** (compare to average)\n",
    "## rep_vote   - 2020 POTUS republican voters* (out of total votes out of pop)\n",
    "## total_vote - 2020 POTUS total voters*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255532f-66e3-45f4-b62c-aaf8f2941744",
   "metadata": {},
   "source": [
    "# GD - Gather Data / RD - Refine Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149febf-9577-46d1-80f9-aa0a37dad8f9",
   "metadata": {},
   "source": [
    "#### GD1 - read census tract geographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9b030f7-3b12-41e6-9dc5-1fe7eabbf20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build/Cache Decision: Cache\n"
     ]
    }
   ],
   "source": [
    "def read_geo_data(directory = 'A_Input/tracts_dbf'):\n",
    "    \n",
    "    ## list dbf files in target directory\n",
    "    dbf_addr = listdir(directory)\n",
    "    dbf_addr = [i for i in dbf_addr if i[-3::] == 'dbf']\n",
    "    \n",
    "    ## define relevant columns from each\n",
    "    desired_columns = {'GEOID': str,\n",
    "                       'STATEFP': str, 'COUNTYFP':str, 'TRACTCE':str,\n",
    "                        'INTPTLAT': float, 'INTPTLON': float, 'ALAND': int}\n",
    "    \n",
    "    ## read in all dbf files\n",
    "    dbf_data = []\n",
    "    for i in dbf_addr:\n",
    "        i_dbf = pd.DataFrame(iter(DBF(directory + '/' + i)))\n",
    "        i_dbf = i_dbf[desired_columns.keys()].astype(desired_columns)\n",
    "        dbf_data.append(i_dbf)\n",
    "        \n",
    "    ## compile data into a single file\n",
    "    dbf_data = pd.concat(dbf_data, axis = 0).sort_values('GEOID')\n",
    "    dbf_data['count'] = 1\n",
    "    dbf_data = dbf_data.reset_index(drop = True)\n",
    "    \n",
    "    ## repair GEOID irregularities\n",
    "    dbf_data['GEOID'] = dbf_data['GEOID'].astype(str).str.zfill(11)\n",
    "    \n",
    "    ## export data\n",
    "    return dbf_data\n",
    "\n",
    "## execute code\n",
    "geo_data = build_or_cache(function = read_geo_data,\n",
    "                          address = 'B_Intermediate/dbf_data.csv.gz',\n",
    "                          permit = set_acceleration['dbf_cache'])\n",
    "log_time('GD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b44dd8-b3a4-4eff-a644-21bb455d388a",
   "metadata": {},
   "source": [
    "#### RD1 - draw a sample from the census tract geographic data and exclude outlier tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3fd248c-6dab-40fe-83a5-2228ac349e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_geo_data(dat = geo_data, too_rural = 20.720e6 * 5,\n",
    "                    too_much = set_acceleration['sample_size']):\n",
    "    \n",
    "    ## filter out extremely rural areas (< 100 people per square mile)\n",
    "    dat = dat.loc[dat.ALAND < too_rural, :]\n",
    "    \n",
    "    ## take systematic sample of the data\n",
    "    i = np.arange(0, dat.shape[0]) % int(1 / too_much)\n",
    "    dat = dat.loc[i == 0, ]\n",
    "    \n",
    "    ## set index\n",
    "    dat = dat.rename({'GEOID':'GEO_ID'}, axis = 1).set_index('GEO_ID')\n",
    "    dat.index = 'ct' + dat.index.astype(str).str.zfill(11)\n",
    "\n",
    "    ## return data\n",
    "    return dat\n",
    "\n",
    "## execute code\n",
    "geo_data = refine_geo_data()\n",
    "log_time('RD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3373db0-4b46-4aac-8b01-9e6d8dfd92da",
   "metadata": {},
   "source": [
    "#### GD2 - read population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28942f36-b9af-41e1-a844-c489d8d20c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pop_data(roster = 'A_Input/sources.csv'):\n",
    "    \n",
    "    ## read in data file roster\n",
    "    roster = pd.read_csv(roster).set_index('OBJ_NAME')\n",
    "    \n",
    "    ## load all datasets in the roster file\n",
    "    pop_data = dict()\n",
    "    for i in roster.index:\n",
    "        var_names = roster.loc[i, 'VAR_NAME'].split(';')\n",
    "        pop_data[i] = pd.read_csv('A_Input' + '/' + roster.loc[i, 'FILE_NAME'],\n",
    "            usecols = var_names, dtype = str)\n",
    "        \n",
    "    ## merge census datasets\n",
    "    census_i = roster.index[roster.SOURCE == 'data.census.gov'].values\n",
    "    census_dat_count = 0\n",
    "    \n",
    "    for i in census_i:\n",
    "        pop_data[i] = pop_data[i].loc[1::, :].set_index('GEO_ID')\n",
    "        if census_dat_count < 1:\n",
    "            pop_data['census'] = pop_data[i]\n",
    "            pop_data.pop(i)\n",
    "            census_dat_count += 1\n",
    "        else:\n",
    "            pop_data['census'] = pop_data['census'].join(pop_data[i])\n",
    "            pop_data.pop(i)\n",
    "            census_dat_count += 1\n",
    "            \n",
    "    ## convert census data to numeric\n",
    "    def robust_int(x):\n",
    "        try: x = int(x)\n",
    "        except: x = 0\n",
    "        return x\n",
    "    map_robust_int = lambda x: x.map(robust_int)\n",
    "    pop_data['census'] = pop_data['census'].apply(map_robust_int)\n",
    "\n",
    "    return pop_data\n",
    "\n",
    "## execute code - see RD2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd744f8f-2948-4bc1-b8f3-61a41011a660",
   "metadata": {},
   "source": [
    "#### RD2 - refine and compile population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ba287d1-300b-42d3-a930-6865bcfab541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build/Cache Decision: Cache\n"
     ]
    }
   ],
   "source": [
    "def refine_pop_data(pop):\n",
    "    \n",
    "    ## -- standardize geographic codes as needed\n",
    "    state_fips = pop.pop('state_fips')# if needed in future; not currently used\n",
    "    pop['census'].index = pop['census'].index.str.replace('1400000US', '')\n",
    "\n",
    "    ## -- refine life expectancy data and merge into census data\n",
    "    pop['lifespan'].columns = ['GEO_ID', 'life_expect']\n",
    "    pop['lifespan'].life_expect = pop['lifespan'].life_expect.astype(float)\n",
    "    pop['census'] = pop['census'].join(pop['lifespan'].set_index('GEO_ID'))\n",
    "    pop.pop('lifespan')\n",
    "    \n",
    "    ## -- refine voting data\n",
    "    \n",
    "    ## filter to necessary data\n",
    "    i = (pop['vote'].party == 'REPUBLICAN') & (pop['vote'].year == '2020')\n",
    "    pop['vote'] = pop['vote'].loc[i].drop(['year', 'party', 'state_po'],\n",
    "                                          axis = 1)\n",
    "    \n",
    "    ## impute total votes (data is irregular from state to state)\n",
    "    temp = pop['vote'].copy()\n",
    "    temp = temp.drop(['totalvotes'], axis = 1)\n",
    "    temp = temp.set_index(['county_fips', 'mode']).astype(int).reset_index()\n",
    "    temp = temp.groupby(['county_fips', 'mode']).sum().reset_index()\n",
    "    total_vote = temp.loc[temp['mode'] == 'TOTAL'].set_index('county_fips')\n",
    "    total_vote = total_vote.drop('mode', axis = 1)\n",
    "    seg_vote = temp.loc[temp['mode'] != 'TOTAL'].groupby('county_fips').sum()\n",
    "    total_vote = pd.concat({'Total': total_vote, 'Alt': seg_vote}, axis = 1)\n",
    "    total_vote = total_vote.max(axis = 1)\n",
    "    total_vote = pd.DataFrame({'repvotes':total_vote})\n",
    "    pop['vote'] = pop['vote'].join(total_vote, on = 'county_fips')\n",
    "    pop['vote'] = pop['vote'].drop_duplicates('county_fips')\n",
    "    pop['vote'] = pop['vote'].drop(['mode', 'candidatevotes'], axis = 1)\n",
    "    del seg_vote, temp, total_vote\n",
    "    \n",
    "    ## calculate percentage voting republican\n",
    "    pop['vote'] = pop['vote'].set_index('county_fips').astype(float)\n",
    "    pop['vote']['reppct'] = pop['vote']['repvotes'] / pop['vote']['totalvotes']\n",
    "    \n",
    "    ## calculate percentage of population that voted\n",
    "    county_total = pd.DataFrame(pop['census']['DP05_0001E'])\n",
    "    county_total['county'] = [i[0:5] for i in county_total.index]\n",
    "    county_total = county_total.groupby('county').sum().astype(int)\n",
    "    pop['vote'] = pop['vote'].join(county_total)\n",
    "    pop['vote']['totalpct'] = pop['vote']['totalvotes'] / pop['vote']['DP05_0001E']\n",
    "    pop['vote'].loc[pop['vote'].totalpct > 1, 'totalpct'] = 159633396 / 331449281\n",
    "    pop['vote'] = pop['vote'][['reppct', 'totalpct']]\n",
    "    \n",
    "    ## merge voting data into census and convert to counts\n",
    "    pop['census']['county'] = [i[0:5] for i in pop['census'].index]\n",
    "    pop['census'] = pop['census'].reset_index().set_index('county').join(pop['vote'])\n",
    "    pop = pop['census'].set_index('GEO_ID')\n",
    "    pop['state'] = [i[0:2] for i in pop.index]\n",
    "    \n",
    "    ## -- impute missing data\n",
    "    \n",
    "    ## impute at the state level\n",
    "    state_mean = pop.copy()[['life_expect', 'reppct', 'totalpct', 'state']]\n",
    "    state_mean = state_mean.groupby('state').mean().round(2)\n",
    "    state_mean = state_mean.loc[pop.state]\n",
    "    for i in state_mean.columns:\n",
    "        j = pop[i].isna().values\n",
    "        pop.loc[j, i] = state_mean.loc[j, i].values\n",
    "    \n",
    "    ## impute at the national level\n",
    "    for i in state_mean.columns:\n",
    "        j = pop[i].isna().values\n",
    "        pop.loc[j, i] = pop[i].mean()\n",
    "    del state_mean\n",
    "    \n",
    "    ## convert vote proportions to counts\n",
    "    pop['rep_vote'] = pop['reppct'] * pop['totalpct'] * pop['DP05_0001E']\n",
    "    pop['rep_vote'] = pop['rep_vote'].round().astype(int)\n",
    "    pop['total_vote'] = (pop['totalpct'] * pop['DP05_0001E']).round().astype(int)\n",
    "    pop = pop.drop(['reppct', 'totalpct', 'state'], axis = 1).round(1)\n",
    "    \n",
    "    ## add prefix to GEO_ID\n",
    "    pop.index = 'ct' + pop.index.astype(str).str.zfill(11)\n",
    "    \n",
    "    ## export result\n",
    "    return pop\n",
    "\n",
    "def read_refine_pop_data():\n",
    "    pop_data = read_pop_data()\n",
    "    pop_data = refine_pop_data(pop = pop_data.copy())\n",
    "    return pop_data\n",
    "    \n",
    "\n",
    "## execute code\n",
    "pop_data = build_or_cache(function = read_refine_pop_data,\n",
    "    address = 'B_Intermediate/pop_data.csv.gz',\n",
    "    permit = set_acceleration['pop_cache'])\n",
    "log_time('RD2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc48382-fdbf-47c5-b7d1-0cbf51cc5691",
   "metadata": {},
   "source": [
    "#### GD3 / RD3 - read and refine text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a2da6a-4fc5-472a-b366-373019afd73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_explanatory_text(addr = 'A_Input/explanation.docx', n = 47):\n",
    "    explain = Document(addr).paragraphs\n",
    "    explain = [txt_wrap(i.text, n) for i in explain]\n",
    "    explain = '\\n'.join(explain)\n",
    "    return explain\n",
    "\n",
    "## execute code\n",
    "explanatory_text = read_explanatory_text()\n",
    "log_time('RD3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15edf0c1-3995-4c01-9364-0630edb65e14",
   "metadata": {},
   "source": [
    "#### RD4 - Reconcile geographic and population dataset tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9390a35a-4e92-4f56-90c3-adae3c2952f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcile_data(geo = geo_data, pop = pop_data):\n",
    "    pop = pop.loc[geo.index]\n",
    "    return geo, pop\n",
    "\n",
    "##  execute code\n",
    "geo_data, pop_data = reconcile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df65672-a010-40b2-94ac-3906eae80072",
   "metadata": {},
   "source": [
    "# Model Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90542624-5a86-4e97-b5a6-1d3c4845dd75",
   "metadata": {},
   "source": [
    "#### MD1 - Precalculate tract-to-tract distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6fb3cca-b253-4240-8d2f-15f99fa62a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build/Cache Decision: Cache\n"
     ]
    }
   ],
   "source": [
    "## define function to do distance compuations in parallel\n",
    "def measure_distance_in_parallel(geo = geo_data):\n",
    "    \n",
    "    xy = list(zip(geo.INTPTLAT.values, geo.INTPTLON.values))\n",
    "    the_iter = list(range(0, len(xy)))\n",
    "    \n",
    "    ## define engine function that will run on each parallel process\n",
    "    def measure_distance_parallel_slice(n, xy_col = xy):\n",
    "        from geopy.distance import geodesic # for parallel process\n",
    "        xy_col = xy_col.copy()\n",
    "        xy_row = xy_col[n]\n",
    "        xy_dist = []\n",
    "        for i in xy_col[0:n]: xy_dist.append(0)\n",
    "        for i in xy_col[n::]:\n",
    "            xy_dist.append(int(round(geodesic(xy_row, i).miles)))\n",
    "        return xy_dist\n",
    "\n",
    "    ## run engine in parallel for each slice of the data\n",
    "    with Cluster(n = set_acceleration['distance_parallel_cores']) as clust:\n",
    "        view = clust.load_balanced_view()\n",
    "        asyncresult = view.map_async(measure_distance_parallel_slice, the_iter)\n",
    "        asyncresult.wait_interactive()\n",
    "        result = asyncresult.get()\n",
    "        \n",
    "    ## package results and export\n",
    "    result = np.array(result)\n",
    "    result = result + result.T\n",
    "    result = pd.DataFrame(result)\n",
    "    result.index, result.columns = (geo.index, geo.index)\n",
    "    return result\n",
    "\n",
    "## execute code\n",
    "tract_distance = build_or_cache(\n",
    "    function = measure_distance_in_parallel,\n",
    "    address = 'B_Intermediate/tract_distance.csv.gz',\n",
    "    permit = set_acceleration['distance_cache'])\n",
    "log_time('MD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc19f1f2-f863-4872-8427-023b5a662713",
   "metadata": {},
   "source": [
    "#### MD2 - Make one-stage agglomeration clustering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "698a3688-72ee-4f26-8487-ec47ad835e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "## score model\n",
    "def agglom_score(clusters, dist):\n",
    "    \n",
    "    ## load libraries (enables parallel processing)\n",
    "    import numpy as np\n",
    "    \n",
    "    ## construct matrix of points that are in the same group\n",
    "    score = clusters.reshape(clusters.shape[0], 1)\n",
    "    score = (score == score.T).astype(int)\n",
    "    \n",
    "    ## sum distance between points in the same cluster\n",
    "    score = (dist * score).sum().sum()\n",
    "    return score.astype(int)\n",
    "\n",
    "## generate partition of census tracts into k clusters based on proximity\n",
    "def agglom_ml(k, dist, score_func = agglom_score):\n",
    "    \n",
    "    ## load libraries (enables parallel processing)\n",
    "    from sklearn.cluster import AgglomerativeClustering # for parallel process\n",
    "    from numpy           import append\n",
    "    \n",
    "    ## divide census tracts into clusters\n",
    "    ml = AgglomerativeClustering(n_clusters = k,\n",
    "                                 affinity = 'precomputed',\n",
    "                                 linkage = 'average',\n",
    "                                 compute_full_tree = False)\n",
    "    ml_clusters = ml.fit_predict(dist)\n",
    "    \n",
    "    ## score the quality of the cluster solution\n",
    "    ml_score = score_func(ml_clusters, dist)\n",
    "    ml_clusters = append(ml_clusters, ml_score)\n",
    "    \n",
    "    ## export results\n",
    "    return ml_clusters\n",
    "\n",
    "## find best solution (using the fit curve 'elbow' approach)\n",
    "def find_best_cluster_solution(cluster_batch):\n",
    "\n",
    "    ## extract x and y\n",
    "    x = cluster_batch.columns.values\n",
    "    y = cluster_batch.iloc[cluster_batch.shape[0] - 1, :].values\n",
    "    \n",
    "    ## regularize x and y\n",
    "    x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "    y = (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "    x = x.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    ## generate reference x and y\n",
    "    x_ref = np.arange(0, 1, 0.01)\n",
    "    x_ref = x_ref.reshape(-1, 1)\n",
    "    y_ref = 1 - x_ref\n",
    "    x_ref = x_ref\n",
    "    \n",
    "    ## calculate distances between xy and xy_ref\n",
    "    x_dist = (x - x_ref.T)**2\n",
    "    y_dist = (y - y_ref.T)**2\n",
    "    elbow_score = x_dist + y_dist\n",
    "    del x_dist, y_dist, x, y, x_ref, y_ref\n",
    "    \n",
    "    ## find the solution that is furthest from the line of equality (the elbow)\n",
    "    elbow_score = elbow_score.min(axis = 1)\n",
    "    elbow_score = (elbow_score == np.max(elbow_score)).astype(int)\n",
    "    \n",
    "    ## package and export\n",
    "    elbow_score = pd.DataFrame({'Elbow':elbow_score}).T\n",
    "    elbow_score.columns = cluster_batch.columns\n",
    "    cluster_batch = pd.concat([cluster_batch, elbow_score], axis = 0)\n",
    "    \n",
    "    ##  return object\n",
    "    return cluster_batch\n",
    "    \n",
    "## fit clusters in parallel for k = 2 through 50\n",
    "def agglom_batch(k_min = 2, k_max = 48, f = agglom_ml,\n",
    "                 dist_mat = tract_distance,\n",
    "                 cores = set_acceleration['cluster_parallel_cores']):\n",
    "    \n",
    "    ## bound check parameters and construct model list\n",
    "    k_min = max(k_min, 2)\n",
    "    k_max = max(k_min + 1, k_max) + 1\n",
    "    k_range = list(range(k_min, k_max))\n",
    "    \n",
    "    ## set distance matrix default\n",
    "    from functools import partial\n",
    "    f = partial(f, dist = dist_mat)\n",
    "    \n",
    "    ## run models in parallel\n",
    "    with Cluster(n = cores) as clust:\n",
    "        view = clust.load_balanced_view()\n",
    "        asyncresult = view.map_async(f, k_range)\n",
    "        asyncresult.wait_interactive()\n",
    "        result = asyncresult.get()\n",
    "    \n",
    "    ## package results and identity best solution\n",
    "    result = pd.DataFrame(np.array(result).T)\n",
    "    result.columns = k_range\n",
    "    result.index = np.append(dist_mat.index, 'Fit')\n",
    "    result = find_best_cluster_solution(result)\n",
    "    result = pd.DataFrame(result, columns = k_range)\n",
    "    return result\n",
    "\n",
    "## test code\n",
    "cluster_level_one = build_or_cache(\n",
    "    function = agglom_batch,\n",
    "    address = 'B_Intermediate/cluster_level_one.csv.gz',\n",
    "    permit = set_acceleration['cluster_L1_cache'])\n",
    "log_time('MD2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceac255-b803-4fd3-a016-63a5473ef61b",
   "metadata": {},
   "source": [
    "#### MD3 - Conduct two-stage agglomeration clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20f5eba1-fa93-4217-9df6-626a48df66d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "def extract_cluster_solution(cl1):\n",
    "    i = cl1.loc['Elbow', ].astype(bool)\n",
    "    cl1 = cl1.loc[ ~cl1.index.isin(['Fit', 'Elbow']), i].squeeze()\n",
    "    return cl1\n",
    "\n",
    "def agglom_l2(dist = tract_distance, func = agglom_batch,\n",
    "              cl1 = cluster_level_one, ecs = extract_cluster_solution):\n",
    "    \n",
    "    ## extract level one partition and set max number of level two clusters\n",
    "    cl1 = ecs(cl1)\n",
    "    max_cl2_clusters = int(48 / max(cl1)) * 2\n",
    "    \n",
    "    ## generate level two clusters for the cities in each level one cluster\n",
    "    cl2 = dict()\n",
    "    for i in set(cl1):\n",
    "        \n",
    "        ## calculate 2nd level cluster solutions for each cluster\n",
    "        cluster_tracts = cl1[cl1 == i].index\n",
    "        dist_iter = dist.loc[cluster_tracts, cluster_tracts].copy()\n",
    "        k_max_iter = min(max_cl2_clusters, dist.shape[0])\n",
    "        cl2[i] = func(dist_mat = dist_iter, k_max = k_max_iter)\n",
    "        \n",
    "        ## define useful indexes\n",
    "        only_cluster = ~cl2[i].index.isin(['Fit', 'Elbow'])\n",
    "        best_elbow = cl2[i].loc['Elbow'] == 1\n",
    "        \n",
    "        ## extract elbow statistics\n",
    "        #elbow_range = best_elbow.copy()\n",
    "        #elbow_range.iat[0] = True\n",
    "        #elbow_range.iat[elbow_range.shape[0]-1] = True\n",
    "        stats = [cl2[i].loc[~only_cluster, ]]\n",
    "        stat_container = [np.nan for k in range(0, cl2[i].shape[0]-2)]\n",
    "        stat_container[0] = stats\n",
    "\n",
    "        ## extract the best L2 cluster assignments for each L1 cluster\n",
    "        cl2[i] = cl2[i].loc[only_cluster, best_elbow]\n",
    "        cl2[i].columns = ['l2_cluster']\n",
    "        cl2[i]['l1_cluster'] = int(i)\n",
    "        cl2[i]['l2_stats'] = stat_container\n",
    "        \n",
    "    ## consolidate and export data\n",
    "    cl2 = pd.concat(cl2.values(), axis = 0).reset_index()\n",
    "    cl2 = cl2.rename({'index':'GEO_ID'}, axis = 1).set_index('GEO_ID')\n",
    "    return cl2\n",
    "\n",
    "## execute code (Note - stats object is too complex to reassemble correctly.\n",
    "   ## This ok for now as it is only being saved for 'just-in-case' reasons.\n",
    "   ## If needed, it can be converted to a csv-format text string\n",
    "cluster_level_two = build_or_cache(\n",
    "    function = agglom_l2,\n",
    "    address  = 'B_Intermediate/cluster_level_two.csv.gz',\n",
    "    permit   = set_acceleration['cluster_L2_cache']\n",
    "    )\n",
    "geo_data = geo_data.join(cluster_level_two)\n",
    "del cluster_level_two\n",
    "geo_data['l2_index'] = geo_data.l1_cluster.astype(str) +\\\n",
    "    (geo_data.l2_cluster + 65).apply(chr)\n",
    "log_time('MD3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e95783-7f9d-4fc1-b003-793470f42c84",
   "metadata": {},
   "source": [
    "# Calculate Cluster Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e44012-fc80-41ea-b644-49f2d9f81338",
   "metadata": {},
   "source": [
    "#### CSS1 - Aggregate statistics to cluster level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "449db4f6-89a7-49c1-9d00-00e77016f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compile data\n",
    "cluster_data = pop_data.join(geo_data[['INTPTLON', 'INTPTLAT', 'l2_index']])\n",
    "cluster_data = cluster_data.set_index('l2_index')\n",
    "cluster_data = cluster_data.drop(['DP05_0077E'], axis = 1)\n",
    "\n",
    "## transform education into pseudo years of education variable\n",
    "cluster_data['edu_years'] = (cluster_data['DP02_0068E'] * 16)\n",
    "cluster_data['edu_years'] = cluster_data['edu_years'] + (\n",
    "    (cluster_data['DP05_0001E'] - cluster_data['DP02_0068E']) * 12)\n",
    "cluster_data['edu_years'] = (\n",
    "    cluster_data['edu_years'] / cluster_data['DP05_0001E'])\n",
    "\n",
    "## calculate weighted averages and population count sums\n",
    "column_operation = {\n",
    "    'DP02_0068E': 'sum', 'DP03_0088E': 'mean', 'life_expect': 'mean',\n",
    "    'rep_vote': 'sum', 'total_vote': 'sum', 'edu_years': 'mean',\n",
    "    'INTPTLON': 'mean', 'INTPTLAT': 'mean'}\n",
    "\n",
    "for i in column_operation.keys():\n",
    "    if column_operation[i] == 'mean':\n",
    "        cluster_data[i] = cluster_data[i] * cluster_data['DP05_0001E']\n",
    "        \n",
    "cluster_data = cluster_data.groupby('l2_index').sum()\n",
    "\n",
    "for i in column_operation.keys():\n",
    "    if column_operation[i] == 'mean':\n",
    "        cluster_data[i] = cluster_data[i] / cluster_data['DP05_0001E']\n",
    "\n",
    "        \n",
    "## inflate counts to compensate for sampling\n",
    "for i in column_operation.keys():\n",
    "    if column_operation[i] == 'sum':\n",
    "        cluster_data[i] = cluster_data[i] / set_acceleration['sample_size']\n",
    "cluster_data['DP05_0001E'] = ((cluster_data['DP05_0001E']\n",
    "                             ) / set_acceleration['sample_size']).astype(int)\n",
    "\n",
    "## scale to hdi factors\n",
    "cluster_data['hdi_edu'] = ((cluster_data['edu_years'] / 15) + (\n",
    "    cluster_data['edu_years'] / 18)) / 2\n",
    "cluster_data['hdi_inc'] = (np.log(cluster_data['DP03_0088E']) - np.log(100)\n",
    "    ) / (np.log(75e3) - np.log(100))\n",
    "cluster_data['hdi_lif'] = (cluster_data['life_expect'] - 20) / (85 - 20)\n",
    "\n",
    "## calculate hdi\n",
    "cluster_data['hdi'] = (\n",
    "    cluster_data['hdi_edu'] * cluster_data['hdi_inc'] * cluster_data['hdi_edu'])\n",
    "cluster_data['hdi'] = ((cluster_data['hdi'] ** (1/3)) * 1000).astype(int)\n",
    "cluster_data['hdi_label'] = cluster_data['hdi'].astype(str)\n",
    "\n",
    "## clean up columns\n",
    "cluster_data = cluster_data.rename({\n",
    "    'DP02_0068E':'education', 'DP03_0088E':'income',\n",
    "    'DP05_0001E':'population'}, axis = 1)\n",
    "cluster_data = cluster_data.drop(\n",
    "    ['hdi_edu', 'hdi_inc', 'hdi_lif', 'education'], axis = 1)\n",
    "cluster_data['income'] = cluster_data['income'].round(0).astype(int)\n",
    "\n",
    "## \n",
    "cluster_data['rep_pct'] = cluster_data['rep_vote'] / cluster_data['total_vote']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127f950-ff08-4315-a527-81ad7d265452",
   "metadata": {},
   "source": [
    "# Prepare Visualization Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7399d4c-7b28-4412-a896-4955b2b66b0e",
   "metadata": {},
   "source": [
    "#### PVD1 - Generate Tract Centroid Voronoi Decomp. Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1412ba81-f5bc-4cdc-87e1-fa0a7e6e750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_voronoi(xy = geo_data[['INTPTLON', 'INTPTLAT']]):\n",
    "    \n",
    "    ## calculate voronoi decomposition\n",
    "    vor_obj = Voronoi(np.array(xy))\n",
    "    \n",
    "    ## assemble polygons for each point\n",
    "    vor_polys = []\n",
    "    for i in vor_obj.point_region:\n",
    "        j = [x for x in vor_obj.regions[i] if x >= 0]\n",
    "        vor_polys.append(vor_obj.vertices[j, :])\n",
    "            \n",
    "    ## generate valid polygons and return object\n",
    "    for i in range(0, len(vor_polys)):\n",
    "        try: vor_polys[i] = make_valid(Polygon(vor_polys[i]))\n",
    "        except: vor_polys[i] = None\n",
    "    return vor_polys\n",
    "        \n",
    "\n",
    "## execute code\n",
    "tract_polys = calculate_voronoi()\n",
    "log_time('PVD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179d05e-e7e3-40b3-8fd4-65514612a7cf",
   "metadata": {},
   "source": [
    "#### PVD2 - Generate Cluster Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c4676da-dc84-42b5-a6d8-4c62a5f6ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_polys(polys, groups):\n",
    "\n",
    "    ## sort polygons into cluster groups\n",
    "    cluster_poly = dict()\n",
    "    for i in set(groups.values): cluster_poly[i] = []\n",
    "    for i in range(0, len(polys) - 1):\n",
    "        group_name = groups.iat[i]\n",
    "        if polys[i] is not None:\n",
    "            cluster_poly[group_name].append(polys[i])\n",
    "\n",
    "    ## merge cluster group polygons and return\n",
    "    for i in cluster_poly.keys():\n",
    "        cluster_poly[i] = unary_union(cluster_poly[i])\n",
    "\n",
    "    return cluster_poly\n",
    "\n",
    "## execute code\n",
    "clust_polys_l1 = merge_polys(tract_polys, geo_data.l1_cluster)\n",
    "clust_polys_l2 = merge_polys(tract_polys, geo_data.l2_index)\n",
    "log_time('PVD2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68dbd4-26b4-4ee4-8d7b-6ddeb8ea3371",
   "metadata": {},
   "source": [
    "# Render Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a6167-aea1-47fa-89a3-3a3405d9fb0c",
   "metadata": {},
   "source": [
    "#### RV0 - make the basic infrastructure for the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16ac184b-0922-4b57-b5f8-742e0a0763cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "def draw_plot_foundation():\n",
    "    \n",
    "    ## make figure and grid objects\n",
    "    global poster_fig\n",
    "    poster_fig = plt.figure(figsize = (36, 24))\n",
    "    poster_grid  = poster_fig.add_gridspec(4, 6, figure = poster_fig,\n",
    "                                        hspace = 0.01, wspace = 0.01, top = 1,\n",
    "                                        left = 0, right = 1, bottom = 0)\n",
    "    poster_fig.set_facecolor(set_color['OrangeDark'])\n",
    "    \n",
    "    ## define axes\n",
    "    global poster_ax\n",
    "    poster_ax = dict()\n",
    "    poster_ax['explain'] = poster_fig.add_subplot(poster_grid[0:4, 0:1])\n",
    "    #poster_ax['elbow']   = poster_fig.add_subplot(poster_grid[3:4, 0:1])\n",
    "\n",
    "    poster_ax['A<'] = poster_fig.add_subplot(poster_grid[0:2, 4:5])\n",
    "    poster_ax['A>'] = poster_fig.add_subplot(poster_grid[0:2, 5:6])\n",
    "    \n",
    "    poster_ax['-<'] = poster_fig.add_subplot(poster_grid[2:3, 4:5])\n",
    "    poster_ax['->'] = poster_fig.add_subplot(poster_grid[2:3, 5:6])\n",
    "    poster_ax['V<'] = poster_fig.add_subplot(poster_grid[3:4, 4:5])\n",
    "    poster_ax['V>'] = poster_fig.add_subplot(poster_grid[3:4, 5:6])\n",
    "    \n",
    "    poster_ax['map1'] = poster_fig.add_subplot(poster_grid[0:2, 1:4],\n",
    "                                              projection = set_map['map_proj'])\n",
    "    poster_ax['map2'] = poster_fig.add_subplot(poster_grid[2:4, 1:4],\n",
    "                                              projection = set_map['map_proj'])\n",
    "\n",
    "    ## remove axis ticks\n",
    "    for i in poster_ax.keys():\n",
    "        poster_ax[i].tick_params(\n",
    "            bottom = False, top = False, left = False, right = False,\n",
    "            labelbottom = False, labeltop = False, labelleft = False,\n",
    "            labelright = False, color = 'red')\n",
    "        poster_ax[i].set_facecolor(set_color['OrangeBG'])\n",
    "        \n",
    "## execute code\n",
    "draw_plot_foundation()\n",
    "log_time('RV0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdb9ab-81c4-41c6-8374-624cad481852",
   "metadata": {},
   "source": [
    "#### RV1 - draw explanation panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c871d19a-6a5b-4561-adad-1df5fa151c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_explanation(txt = explanatory_text):\n",
    "    poster_ax['explain'].set_ylim(0, 3)\n",
    "    poster_ax['explain'].set_xlim(0, 1)\n",
    "    poster_ax['explain'].text(x = 0.05, y = 2.5, s = txt,\n",
    "        fontweight = 'bold',\n",
    "        horizontalalignment = 'left', verticalalignment = 'top',\n",
    "        fontsize = set_font['small'], color = set_color['OrangeDark'])\n",
    "    \n",
    "render_explanation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b3bed7-8d9a-43d6-a71d-091da66a5140",
   "metadata": {},
   "source": [
    "#### RV2 - draw optimization 'elbow' curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf0029-1941-481e-90af-181cfd29ab5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1600935-4d81-4809-b2aa-21fa53b1da34",
   "metadata": {},
   "source": [
    "#### RV3 - draw cluster map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65311cb6-7c5a-4d9b-8960-47244f43d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- layers\n",
    "## 0 - States\n",
    "## 1 - Lakes\n",
    "## 3 - Tract centroids\n",
    "## 5, 6 - Cluster boundaries\n",
    "## 8 - Ocean\n",
    "## 9 - Cluster labels\n",
    "\n",
    "## draw generic map layers\n",
    "def draw_generic_map(ax):\n",
    "    poster_ax[ax].set_extent(set_map['bounds'])\n",
    "    poster_ax[ax].add_geometries(list(feature.STATES.geometries()),\n",
    "        facecolor = '#33221100', edgecolor = set_color['OrangeLight'],\n",
    "        lw = 1, zorder = 0, crs = PlateCarree())\n",
    "    poster_ax[ax].add_feature(feature.LAKES,\n",
    "        facecolor = set_color['OrangeBG'], edgecolor = set_color['OrangeLight'],\n",
    "        lw = 1, zorder = 1)\n",
    "    poster_ax[ax].add_feature(feature.OCEAN,\n",
    "        facecolor = set_color['OrangeBG'], edgecolor = set_color['OrangeLight'],\n",
    "        lw = 1, zorder = 8)\n",
    "    \n",
    "## draw census tract centroids\n",
    "def draw_census_centroids(ax, geo = geo_data):\n",
    "    poster_ax[ax].scatter(\n",
    "        x = geo_data.INTPTLON.values, y = geo_data.INTPTLAT.values,\n",
    "        color = set_color['OrangeDark'], zorder = 3,\n",
    "        transform = PlateCarree(), s = 1)\n",
    "    \n",
    "## label clusters\n",
    "def label_cluster(ax, label_col = None, dat = cluster_data):\n",
    "\n",
    "    if label_col is None: dat['label'] = dat.index\n",
    "    else: dat['label'] = dat.index + '\\n' + dat[label_col]\n",
    "\n",
    "    bb = dict(edgecolor = set_color['AzureDark'], lw = 3,\n",
    "              facecolor = set_color['AzureBG'] + 'A0', boxstyle = 'round')\n",
    "\n",
    "    for i in dat.index:\n",
    "        poster_ax[ax].text(\n",
    "            x = dat.at[i, 'INTPTLON'], y = dat.at[i, 'INTPTLAT'],\n",
    "            s = dat.at[i, 'label'],\n",
    "            fontsize = set_font['small'], color = set_color['AzureDark'],\n",
    "            transform = PlateCarree(), fontweight = 'bold',\n",
    "            bbox = bb, zorder = 9)\n",
    "\n",
    "## draw cluster boundaries\n",
    "def draw_cluster(ax, poly, the_lw = 2, zo = 5):\n",
    "    for i in poly.keys():\n",
    "        poster_ax[ax].plot(*poly[i].exterior.xy,\n",
    "            zorder = zo, color = set_color['AzureBG'],\n",
    "            transform = PlateCarree(), lw = the_lw * 1.5)\n",
    "        poster_ax[ax].plot(*poly[i].exterior.xy,\n",
    "            zorder = zo + 1, color = set_color['AzureDark'],\n",
    "            transform = PlateCarree(), lw = the_lw)\n",
    "\n",
    "## execute code - map 1\n",
    "draw_generic_map('map1')\n",
    "draw_census_centroids('map1')\n",
    "draw_cluster('map1', clust_polys_l2)\n",
    "\n",
    "## execute code - map 2\n",
    "draw_generic_map('map2')\n",
    "label_cluster('map2', label_col = 'hdi_label')\n",
    "draw_cluster('map2', clust_polys_l2)\n",
    "## draw_cluster('map2', clust_polys_l1, the_lw = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf58ce-b8e8-474f-96ef-e7e3c5937f6f",
   "metadata": {},
   "source": [
    "#### RV4 - bar panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66c2d825-5009-40c7-8cb3-173d4bf3bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bar(ax, major_col, minor_col = None, dat = cluster_data.copy(),\n",
    "             unit = None):\n",
    "    \n",
    "    dat = dat.iloc[::-1].copy()\n",
    "    \n",
    "    ## scale data for visualization purposes\n",
    "    def min_max_scale(x):\n",
    "        x = (x - min(x)) / (max(x) - min(x))\n",
    "        n = 0.82\n",
    "        x = (x * n) + (1 - n)\n",
    "        return x\n",
    "    \n",
    "    dat['scaled_major'] = min_max_scale(dat[major_col])\n",
    "    if minor_col is not None:\n",
    "        dat['pct_minor'] = (dat[minor_col] / dat[major_col])\n",
    "        dat['scaled_minor'] = dat['scaled_major'] * dat['pct_minor']\n",
    "        \n",
    "    ## convert bar labels\n",
    "    dat['label_major'] = dat[major_col]\n",
    "    if unit == '/M':\n",
    "        dat['label_major'] = (dat['label_major'] / 1e6).round(0).astype(int)\n",
    "        dat['label_major'] = dat['label_major'].astype(str) + 'M'\n",
    "        min_display_major = 1.5e6\n",
    "    else:\n",
    "        min_display_major = 0\n",
    "\n",
    "    if minor_col is not None:\n",
    "        dat['label_minor'] = (dat['pct_minor'] * 100).round(0).astype(int)\n",
    "        dat['label_minor'] = dat['label_minor'].astype(str) + '%'\n",
    "          \n",
    "    ## calculate display parameters\n",
    "    bar_ceiling = 0.89\n",
    "    bar_start = 0.10\n",
    "    lab_pad = 0.02\n",
    "    poster_ax[ax].set_xlim(0, 1)\n",
    "    poster_ax[ax].set_ylim(0, 2)\n",
    "    bar_space = 1.9 / (dat.shape[0] + 1)\n",
    "    dat['bar_spacing'] = np.arange(bar_space, 1.9, bar_space)\n",
    "    bb_blue = dict(edgecolor = set_color['AzureDark'], lw = 2,\n",
    "             facecolor = set_color['AzureBG'] + '80', boxstyle = 'square')\n",
    "    bb_orange = dict(edgecolor = set_color['OrangeMedium'], lw = 2,\n",
    "             facecolor = set_color['OrangeLight'], boxstyle = 'square')\n",
    "\n",
    "    ## draw major bar\n",
    "    poster_ax[ax].barh(\n",
    "        y = dat['bar_spacing'], height = bar_space,\n",
    "        width = dat['scaled_major'] * bar_ceiling,\n",
    "        color = set_color['OrangeLight'], edgecolor = set_color['OrangeDark'],\n",
    "        lw = 2, left = bar_start)\n",
    "    \n",
    "    ## draw bar labels\n",
    "    for i in dat.index:\n",
    "        poster_ax[ax].text(\n",
    "            y = dat.loc[i, 'bar_spacing'], x = lab_pad, s = i,\n",
    "            horizontalalignment = 'left', verticalalignment = 'center',\n",
    "            fontsize = set_font['small'], fontweight = 'bold',\n",
    "            color = set_color['OrangeDark'])\n",
    "    \n",
    "    ## draw minor bar (if applicable) and label\n",
    "    if minor_col is not None:\n",
    "        poster_ax[ax].barh(y = dat['bar_spacing'], height = bar_space,\n",
    "            width = dat['scaled_minor'] * bar_ceiling,\n",
    "            color = set_color['OrangeMedium'],\n",
    "            edgecolor = set_color['OrangeDark'], lw = 2, left = bar_start)\n",
    "        \n",
    "        for i in dat.index:\n",
    "            poster_ax[ax].text(\n",
    "                x = bar_start, y = dat.loc[i, 'bar_spacing'],\n",
    "                s = dat.loc[i, 'label_minor'],\n",
    "                fontsize = set_font['small'], fontweight = 'bold',\n",
    "                color = set_color['OrangeLight'],\n",
    "                horizontalalignment = 'left', verticalalignment = 'center'\n",
    "                )\n",
    "        \n",
    "    ## label major bars\n",
    "    for i in dat.index:\n",
    "        if dat.loc[i, major_col] > min_display_major:\n",
    "            poster_ax[ax].text(\n",
    "                y = dat.loc[i, 'bar_spacing'],\n",
    "                x = bar_start + dat.loc[i, 'scaled_major'] * (\n",
    "                    bar_ceiling - 0.01),\n",
    "                s = dat.loc[i, 'label_major'],\n",
    "                horizontalalignment = 'right', verticalalignment = 'center',\n",
    "                fontsize = set_font['small'], fontweight = 'bold',\n",
    "                color = set_color['OrangeDark'])\n",
    "\n",
    "## execute code\n",
    "draw_bar('A<', 'hdi')\n",
    "draw_bar('A>', 'total_vote', minor_col = 'rep_vote', unit = '/M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77caf8-a155-415f-996b-c995e5e4cf23",
   "metadata": {},
   "source": [
    "#### RV5 - scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "902e7bd8-d1df-416a-82a5-3c52df3fbf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid_distance(xy1, xy2):\n",
    "    x1 = np.array(xy1.iloc[:, 0]).reshape((-1, 1))\n",
    "    y1 = np.array(xy1.iloc[:, 1]).reshape((-1, 1))\n",
    "    x2 = np.array(xy2.iloc[:, 0]).reshape((1, -1))\n",
    "    y2 = np.array(xy2.iloc[:, 1]).reshape((1, -1))\n",
    "    d = ((x1 - x2)**2 + (y1 - y2)**2)**(1/2)\n",
    "    return d\n",
    "\n",
    "def space_scatter(x, y, space = 0.05, dat = cluster_data.copy()):\n",
    "    \n",
    "    ## generate a point grid\n",
    "    x_grid = np.arange(min(dat[x]), max(dat[x]), space)\n",
    "    y_grid = np.arange(min(dat[y]), max(dat[y]), space)\n",
    "    xy_grid = np.meshgrid(x_grid, y_grid)\n",
    "    for i in range(0, 2): xy_grid[i] = xy_grid[i].reshape((-1, 1))\n",
    "    xy_grid = np.concatenate(xy_grid, axis = 1)\n",
    "    xy_grid = pd.DataFrame(xy_grid, columns = ['x', 'y'])\n",
    "    xy_grid['Link'] = -1\n",
    "    \n",
    "    ## calculate distance between grid points and xy\n",
    "    xy_dist = euclid_distance(dat[[x, y]], xy_grid[['x','y']])\n",
    "    \n",
    "    ## assign points to closest grid point\n",
    "    for i in range(0, dat.shape[0]):\n",
    "        j = int(np.nonzero(xy_dist[i, ] == min(xy_dist[i, ]))[0][0])\n",
    "        xy_grid.loc[j, 'Link'] = i\n",
    "        xy_dist[:, j] = 999e6\n",
    "        \n",
    "    ## extract new coordinates\n",
    "    xy_grid = xy_grid.loc[ xy_grid.Link != -1, :].sort_values('Link')\n",
    "    xy_grid.index = dat.index\n",
    "    return xy_grid\n",
    "\n",
    "\n",
    "def draw_scatter(ax, x, y, dat = cluster_data.copy()):\n",
    "    \n",
    "    ## set plot limits and bbox parameters\n",
    "    poster_ax[ax].set_xlim(0, 1)\n",
    "    poster_ax[ax].set_ylim(0, 1)\n",
    "    bb = dict(edgecolor = set_color['OrangeDark'], lw = 2,\n",
    "                facecolor = set_color['OrangeBG'], boxstyle = 'square')\n",
    "    \n",
    "    ## scale coordinates\n",
    "    def min_max_scale(x, the_max):\n",
    "        x = (x - min(x)) / (max(x) - min(x))\n",
    "        x = (x * the_max) + 0.10\n",
    "        return x\n",
    "    dat['x_scaled'] = min_max_scale(dat[x], 0.80)\n",
    "    dat['y_scaled'] = min_max_scale(dat[y], 0.77)\n",
    "    \n",
    "    ## disambiguate points\n",
    "    spaced_xy = space_scatter('x_scaled', 'y_scaled', dat = dat)\n",
    "    \n",
    "    ## plot points\n",
    "    poster_ax[ax].scatter(spaced_xy['x'], spaced_xy['y'], color = '#33221100')\n",
    "    for i in spaced_xy.index:\n",
    "        poster_ax[ax].text(color = set_color['OrangeDark'], bbox = bb,\n",
    "            x = spaced_xy.loc[i, 'x'], y = spaced_xy.loc[i, 'y'],\n",
    "            s = i, fontweight = 'bold', fontsize = set_font['small'] - 3)\n",
    "        \n",
    "def draw_scatter_labels(ax, x_label, y_label):\n",
    "    poster_ax[ax].text(x = 0.01, y = 0.5, s = y_label,\n",
    "            fontsize = set_font['medium'], rotation = 90,\n",
    "            color = set_color['OrangeDark'], fontweight = 'bold',\n",
    "            horizontalalignment = 'left', verticalalignment = 'center')\n",
    "    poster_ax[ax].text(x = 0.5, y = 0.01, s = x_label,\n",
    "            fontsize = set_font['medium'],\n",
    "            color = set_color['OrangeDark'], fontweight = 'bold',\n",
    "            horizontalalignment = 'center', verticalalignment = 'bottom')\n",
    "\n",
    "\n",
    "## execute code\n",
    "draw_scatter('-<', 'income', 'edu_years')\n",
    "draw_scatter_labels('-<', 'Income Per Capita', 'Average Years Of Education')\n",
    "\n",
    "draw_scatter('->', 'income', 'life_expect')\n",
    "draw_scatter_labels('->', 'Income Per Capita', 'Average Life Expectancy')\n",
    "\n",
    "draw_scatter('V<', 'edu_years', 'life_expect')\n",
    "draw_scatter_labels('V<', 'Average Years Of Education',\n",
    "                    'Average Life Expectancy')\n",
    "\n",
    "draw_scatter('V>', 'rep_pct', 'hdi')\n",
    "draw_scatter_labels('V>', 'Percentage Voting Republican',\n",
    "                    'Human Development Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9405563-96c9-43d7-a8ea-ef049acd492b",
   "metadata": {},
   "source": [
    "#### RV6 - Plot Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0914ce73-b620-451b-914f-8b4199056050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_title(ax, height, title):\n",
    "    poster_ax[ax].text(\n",
    "        x = 0.99, y = height, s = title,\n",
    "        fontsize = set_font['medium'], fontweight = 'bold',\n",
    "        color = set_color['OrangeDark'],\n",
    "        horizontalalignment = 'right', verticalalignment = 'top')\n",
    "    \n",
    "## execute code\n",
    "make_title('explain', 2.99,\n",
    "    'What Alternative State Borders' + '\\n' +\\\n",
    "    'Would Consistently Put People'  + '\\n' +\\\n",
    "    'Who Live Near Each Other In The'+ '\\n' +\\\n",
    "    'Same State?' + '\\n\\n' +\\\n",
    "    'How Would Quality of Life And'  + '\\n' +\\\n",
    "    'Politics Vary Between The New'  + '\\n' +\\\n",
    "    'States?'\n",
    "          )\n",
    "##make_title('elbow',   0.99, 'Optimization Curve\\nFor Cluster Assignments')\n",
    "\n",
    "\n",
    "make_title('A<', 1.99, 'HDI Score For Each Cluster')\n",
    "make_title('A>', 1.99, 'Total Voters And Percentage\\nRepublican Voters in 2020')\n",
    "\n",
    "make_title('-<', 0.99, 'Correspondence Between\\nIncome And Education')\n",
    "make_title('->', 0.99, 'Correspondence Between\\nIncome And Life Expectancy')\n",
    "make_title('V<', 0.99, 'Correspondence Between\\nEducation And Life Expectancy')\n",
    "make_title('V>', 0.99,\n",
    "    'Correspondence Between Percent\\nRepublican Voters And HDI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b648d2-2c43-48a9-9520-60482c4f46fd",
   "metadata": {},
   "source": [
    "# Footer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe42104-9eb2-47be-8050-1f1d1125eede",
   "metadata": {},
   "source": [
    "#### write poster to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2860728-2bc2-401e-a043-c037c8752604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_poster():\n",
    "    poster_fig.savefig('C_Output/pop_cluster_map.png')\n",
    "    poster_fig.savefig('C_Output/pop_cluster_map.pdf')\n",
    "save_poster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "257b6438-6ef8-4ea2-b4d8-a5039053e607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time log:\n",
      "   H2: 14:44:15\n",
      "   H3: 14:44:15\n",
      "  GD1: 14:44:15\n",
      "  RD1: 14:44:15\n",
      "  RD2: 14:44:15\n",
      "  RD3: 14:44:15\n",
      "  MD1: 14:44:22\n",
      "  MD2: 14:44:22\n",
      "  MD3: 14:44:22\n",
      " PVD1: 14:44:22\n",
      " PVD2: 14:44:23\n",
      "  RV0: 14:44:23\n",
      "  End: 14:44:29\n"
     ]
    }
   ],
   "source": [
    "log_time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
