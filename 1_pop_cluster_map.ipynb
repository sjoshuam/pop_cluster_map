{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fb3dd83-1789-477c-948e-6c0debd89b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84ff307-1f5b-4341-9bd9-90d82b9a60b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# H - Header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2fd9fc-433c-48fe-a986-f74fb1c692e4",
   "metadata": {},
   "source": [
    "#### H1 - Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2893cbc5-099e-469b-88d6-3481d734115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard foundational libraries\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## import specific functions\n",
    "from matplotlib.colors  import hsv_to_rgb, to_hex\n",
    "from os                 import mkdir, listdir\n",
    "from os.path            import isfile, isdir\n",
    "from datetime           import datetime, timedelta\n",
    "from cartopy            import feature\n",
    "from cartopy.crs        import LambertConformal, PlateCarree\n",
    "from cartopy.io import shapereader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "from dbfread            import DBF\n",
    "from docx               import Document\n",
    "from textwrap           import fill as txt_wrap\n",
    "from geopy.distance     import geodesic\n",
    "from ipyparallel        import Cluster\n",
    "from sklearn.cluster    import AgglomerativeClustering\n",
    "from functools          import partial\n",
    "from scipy.spatial      import Voronoi\n",
    "from shapely.geometry   import Polygon\n",
    "from shapely.ops        import unary_union\n",
    "from shapely.validation import make_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e46a131-c865-43fc-80be-1c2c1576466f",
   "metadata": {},
   "source": [
    "#### H2 - Basic Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e061a3d-8cb6-4bd5-8f67-881c337b1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up standard directories if needed\n",
    "def make_standard_file_system():\n",
    "    for i in ['A_Input', 'B_Intermediate', 'C_Output']:\n",
    "        if not isdir(i): mkdir(i)\n",
    "\n",
    "## log time elapsed\n",
    "time_log = dict()\n",
    "def log_time(the_id = 'End Log'):\n",
    "    \n",
    "    ## construct new time stamp\n",
    "    now_time = str(datetime.now().hour).zfill(2)\n",
    "    now_time = now_time +':'+ str(datetime.now().minute).zfill(2)\n",
    "    now_time = now_time +':'+ str(datetime.now().second).zfill(2)\n",
    "\n",
    "    ## add to time log\n",
    "    if the_id == 'End Log':\n",
    "        time_log['End'] = now_time\n",
    "        print('Time log:')\n",
    "        for i in time_log.keys():\n",
    "            print(i.rjust(5) + ':', time_log[i])\n",
    "    else:\n",
    "        time_log[the_id] = now_time\n",
    "        \n",
    "## toggle cache versus build\n",
    "def build_or_cache(function, address, permit):\n",
    "    if permit and isfile(address):\n",
    "        print('Build/Cache Decision: Cache')\n",
    "        the_file = pd.read_csv(address, index_col = 0)\n",
    "    else:\n",
    "        print('Build/Cache Decision: Build')\n",
    "        the_file = function()\n",
    "        the_file.to_csv(address)\n",
    "    return the_file\n",
    "    \n",
    "## execute functions\n",
    "make_standard_file_system()\n",
    "log_time('H2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12f165-5257-40f4-b625-5950dac2d955",
   "metadata": {},
   "source": [
    "#### H3 - Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9577a3a0-6eb6-4a48-a510-a8d779269232",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set color palette\n",
    "set_color = {\n",
    "    'AzureDark'    :(7/12, 1.0, 0.4),\n",
    "    'AzureMedium'  :(7/12, 0.7, 0.7),\n",
    "    'AzureLight'   :(7/12, 0.4, 1.0),\n",
    "    'AzureBG'      :(7/12, 0.1, 1.0),\n",
    "    \n",
    "    'OrangeDark'   :(1/12, 1.0, 0.4),\n",
    "    'OrangeMedium' :(1/12, 0.7, 0.7),\n",
    "    'OrangeLight'  :(1/12, 0.4, 1.0),\n",
    "    'OrangeBG'     :(1/12, 0.1, 1.0)\n",
    "    }\n",
    "for i in set_color.keys(): set_color[i] = to_hex(hsv_to_rgb(set_color[i]))\n",
    "\n",
    "## set font sizes\n",
    "set_font = {\n",
    "    'small' : 15,\n",
    "    'medium': 20,\n",
    "    'large' : 25\n",
    "    }\n",
    "\n",
    "## time-saver settings\n",
    "set_acceleration = {\n",
    "    'dbf_cache'              : True,\n",
    "    'sample_size'            : 1/5, ## 1/5, 1/10, 1/20\n",
    "    'pop_cache'              : True,\n",
    "    'distance_parallel_cores': 8,\n",
    "    'distance_cache'         : True,\n",
    "    'cluster_parallel_cores' : 8,\n",
    "    'cluster_L1_cache'       : True,\n",
    "    'cluster_L2_cache'       : True\n",
    "    }\n",
    "\n",
    "## map parameters\n",
    "set_map = {\n",
    "    'bounds'  : [-124.73 + 5, -66.95 - 5, 25.12 - 3.3, 49.38 + 3.3],\n",
    "    'map_proj': LambertConformal(\n",
    "        central_longitude = (-124.73 - 66.95) / 2,\n",
    "        central_latitude = (25.12 + 49.38) / 2,\n",
    "        standard_parallels = (25.12, 49.38))\n",
    "    }\n",
    "log_time('H3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048528d8-d27b-4549-bbcf-22c65d8d2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- variable codes codes\n",
    "## DP02_0068E - Bachlor's Degree* (out of total pop)\n",
    "## DP03_0088E - Income Per Capita** (compare to average)\n",
    "## DP05_0001E - Total Population*\n",
    "## DP05_0077E - Non-Hispanic White\n",
    "## life_expect - life expectancy** (compare to average)\n",
    "## rep_vote   - 2020 POTUS republican voters* (out of total votes out of pop)\n",
    "## total_vote - 2020 POTUS total voters*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255532f-66e3-45f4-b62c-aaf8f2941744",
   "metadata": {},
   "source": [
    "# GD - Gather Data / RD - Refine Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149febf-9577-46d1-80f9-aa0a37dad8f9",
   "metadata": {},
   "source": [
    "#### GD1 - read census tract geographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9b030f7-3b12-41e6-9dc5-1fe7eabbf20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build/Cache Decision: Cache\n"
     ]
    }
   ],
   "source": [
    "def read_geo_data(directory = 'A_Input/tracts_dbf'):\n",
    "    \n",
    "    ## list dbf files in target directory\n",
    "    dbf_addr = listdir(directory)\n",
    "    dbf_addr = [i for i in dbf_addr if i[-3::] == 'dbf']\n",
    "    \n",
    "    ## define relevant columns from each\n",
    "    desired_columns = {'GEOID': str,\n",
    "                       'STATEFP': str, 'COUNTYFP':str, 'TRACTCE':str,\n",
    "                        'INTPTLAT': float, 'INTPTLON': float, 'ALAND': int}\n",
    "    \n",
    "    ## read in all dbf files\n",
    "    dbf_data = []\n",
    "    for i in dbf_addr:\n",
    "        i_dbf = pd.DataFrame(iter(DBF(directory + '/' + i)))\n",
    "        i_dbf = i_dbf[desired_columns.keys()].astype(desired_columns)\n",
    "        dbf_data.append(i_dbf)\n",
    "        \n",
    "    ## compile data into a single file\n",
    "    dbf_data = pd.concat(dbf_data, axis = 0).sort_values('GEOID')\n",
    "    dbf_data['count'] = 1\n",
    "    dbf_data = dbf_data.reset_index(drop = True)\n",
    "    \n",
    "    ## repair GEOID irregularities\n",
    "    dbf_data['GEOID'] = dbf_data['GEOID'].astype(str).str.zfill(11)\n",
    "    \n",
    "    ## export data\n",
    "    return dbf_data\n",
    "\n",
    "## execute code\n",
    "geo_data = build_or_cache(function = read_geo_data,\n",
    "                          address = 'B_Intermediate/dbf_data.csv.gz',\n",
    "                          permit = set_acceleration['dbf_cache'])\n",
    "log_time('GD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b44dd8-b3a4-4eff-a644-21bb455d388a",
   "metadata": {},
   "source": [
    "#### RD1 - draw a sample from the census tract geographic data and exclude outlier tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3fd248c-6dab-40fe-83a5-2228ac349e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_geo_data(dat = geo_data, too_rural = 20.720e6 * 5,\n",
    "                    too_much = set_acceleration['sample_size']):\n",
    "    \n",
    "    ## filter out extremely rural areas (< 100 people per square mile)\n",
    "    dat = dat.loc[dat.ALAND < too_rural, :]\n",
    "    \n",
    "    ## take systematic sample of the data\n",
    "    i = np.arange(0, dat.shape[0]) % int(1 / too_much)\n",
    "    dat = dat.loc[i == 0, ]\n",
    "    \n",
    "    ## set index\n",
    "    dat = dat.rename({'GEOID':'GEO_ID'}, axis = 1).set_index('GEO_ID')\n",
    "    dat.index = 'ct' + dat.index.astype(str).str.zfill(11)\n",
    "\n",
    "    ## return data\n",
    "    return dat\n",
    "\n",
    "## execute code\n",
    "geo_data = refine_geo_data()\n",
    "log_time('RD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3373db0-4b46-4aac-8b01-9e6d8dfd92da",
   "metadata": {},
   "source": [
    "#### GD2 - read population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28942f36-b9af-41e1-a844-c489d8d20c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pop_data(roster = 'A_Input/sources.csv'):\n",
    "    \n",
    "    ## read in data file roster\n",
    "    roster = pd.read_csv(roster).set_index('OBJ_NAME')\n",
    "    \n",
    "    ## load all datasets in the roster file\n",
    "    pop_data = dict()\n",
    "    for i in roster.index:\n",
    "        var_names = roster.loc[i, 'VAR_NAME'].split(';')\n",
    "        pop_data[i] = pd.read_csv('A_Input' + '/' + roster.loc[i, 'FILE_NAME'],\n",
    "            usecols = var_names, dtype = str)\n",
    "        \n",
    "    ## merge census datasets\n",
    "    census_i = roster.index[roster.SOURCE == 'data.census.gov'].values\n",
    "    census_dat_count = 0\n",
    "    \n",
    "    for i in census_i:\n",
    "        pop_data[i] = pop_data[i].loc[1::, :].set_index('GEO_ID')\n",
    "        if census_dat_count < 1:\n",
    "            pop_data['census'] = pop_data[i]\n",
    "            pop_data.pop(i)\n",
    "            census_dat_count += 1\n",
    "        else:\n",
    "            pop_data['census'] = pop_data['census'].join(pop_data[i])\n",
    "            pop_data.pop(i)\n",
    "            census_dat_count += 1\n",
    "            \n",
    "    ## convert census data to numeric\n",
    "    def robust_int(x):\n",
    "        try: x = int(x)\n",
    "        except: x = 0\n",
    "        return x\n",
    "    map_robust_int = lambda x: x.map(robust_int)\n",
    "    pop_data['census'] = pop_data['census'].apply(map_robust_int)\n",
    "\n",
    "    return pop_data\n",
    "\n",
    "## execute code - see RD2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd744f8f-2948-4bc1-b8f3-61a41011a660",
   "metadata": {},
   "source": [
    "#### RD2 - refine and compile population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ba287d1-300b-42d3-a930-6865bcfab541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build/Cache Decision: Cache\n"
     ]
    }
   ],
   "source": [
    "def refine_pop_data(pop):\n",
    "    \n",
    "    ## -- standardize geographic codes as needed\n",
    "    state_fips = pop.pop('state_fips')# if needed in future; not currently used\n",
    "    pop['census'].index = pop['census'].index.str.replace('1400000US', '')\n",
    "\n",
    "    ## -- refine life expectancy data and merge into census data\n",
    "    pop['lifespan'].columns = ['GEO_ID', 'life_expect']\n",
    "    pop['lifespan'].life_expect = pop['lifespan'].life_expect.astype(float)\n",
    "    pop['census'] = pop['census'].join(pop['lifespan'].set_index('GEO_ID'))\n",
    "    pop.pop('lifespan')\n",
    "    \n",
    "    ## -- refine voting data\n",
    "    \n",
    "    ## filter to necessary data\n",
    "    i = (pop['vote'].party == 'REPUBLICAN') & (pop['vote'].year == '2020')\n",
    "    pop['vote'] = pop['vote'].loc[i].drop(['year', 'party', 'state_po'],\n",
    "                                          axis = 1)\n",
    "    \n",
    "    ## impute total votes (data is irregular from state to state)\n",
    "    temp = pop['vote'].copy()\n",
    "    temp = temp.drop(['totalvotes'], axis = 1)\n",
    "    temp = temp.set_index(['county_fips', 'mode']).astype(int).reset_index()\n",
    "    temp = temp.groupby(['county_fips', 'mode']).sum().reset_index()\n",
    "    total_vote = temp.loc[temp['mode'] == 'TOTAL'].set_index('county_fips')\n",
    "    total_vote = total_vote.drop('mode', axis = 1)\n",
    "    seg_vote = temp.loc[temp['mode'] != 'TOTAL'].groupby('county_fips').sum()\n",
    "    total_vote = pd.concat({'Total': total_vote, 'Alt': seg_vote}, axis = 1)\n",
    "    total_vote = total_vote.max(axis = 1)\n",
    "    total_vote = pd.DataFrame({'repvotes':total_vote})\n",
    "    pop['vote'] = pop['vote'].join(total_vote, on = 'county_fips')\n",
    "    pop['vote'] = pop['vote'].drop_duplicates('county_fips')\n",
    "    pop['vote'] = pop['vote'].drop(['mode', 'candidatevotes'], axis = 1)\n",
    "    del seg_vote, temp, total_vote\n",
    "    \n",
    "    ## calculate percentage voting republican\n",
    "    pop['vote'] = pop['vote'].set_index('county_fips').astype(float)\n",
    "    pop['vote']['reppct'] = pop['vote']['repvotes'] / pop['vote']['totalvotes']\n",
    "    \n",
    "    ## calculate percentage of population that voted\n",
    "    county_total = pd.DataFrame(pop['census']['DP05_0001E'])\n",
    "    county_total['county'] = [i[0:5] for i in county_total.index]\n",
    "    county_total = county_total.groupby('county').sum().astype(int)\n",
    "    pop['vote'] = pop['vote'].join(county_total)\n",
    "    pop['vote']['totalpct'] = pop['vote']['totalvotes'] / pop['vote']['DP05_0001E']\n",
    "    pop['vote'].loc[pop['vote'].totalpct > 1, 'totalpct'] = 159633396 / 331449281\n",
    "    pop['vote'] = pop['vote'][['reppct', 'totalpct']]\n",
    "    \n",
    "    ## merge voting data into census and convert to counts\n",
    "    pop['census']['county'] = [i[0:5] for i in pop['census'].index]\n",
    "    pop['census'] = pop['census'].reset_index().set_index('county').join(pop['vote'])\n",
    "    pop = pop['census'].set_index('GEO_ID')\n",
    "    pop['state'] = [i[0:2] for i in pop.index]\n",
    "    \n",
    "    ## -- impute missing data\n",
    "    \n",
    "    ## impute at the state level\n",
    "    state_mean = pop.copy()[['life_expect', 'reppct', 'totalpct', 'state']]\n",
    "    state_mean = state_mean.groupby('state').mean().round(2)\n",
    "    state_mean = state_mean.loc[pop.state]\n",
    "    for i in state_mean.columns:\n",
    "        j = pop[i].isna().values\n",
    "        pop.loc[j, i] = state_mean.loc[j, i].values\n",
    "    \n",
    "    ## impute at the national level\n",
    "    for i in state_mean.columns:\n",
    "        j = pop[i].isna().values\n",
    "        pop.loc[j, i] = pop[i].mean()\n",
    "    del state_mean\n",
    "    \n",
    "    ## convert vote proportions to counts\n",
    "    pop['rep_vote'] = pop['reppct'] * pop['totalpct'] * pop['DP05_0001E']\n",
    "    pop['rep_vote'] = pop['rep_vote'].round().astype(int)\n",
    "    pop['total_vote'] = (pop['totalpct'] * pop['DP05_0001E']).round().astype(int)\n",
    "    pop = pop.drop(['reppct', 'totalpct', 'state'], axis = 1).round(1)\n",
    "    \n",
    "    ## add prefix to GEO_ID\n",
    "    pop.index = 'ct' + pop.index.astype(str).str.zfill(11)\n",
    "    \n",
    "    ## export result\n",
    "    return pop\n",
    "\n",
    "def read_refine_pop_data():\n",
    "    pop_data = read_pop_data()\n",
    "    pop_data = refine_pop_data(pop = pop_data.copy())\n",
    "    return pop_data\n",
    "    \n",
    "\n",
    "## execute code\n",
    "pop_data = build_or_cache(function = read_refine_pop_data,\n",
    "    address = 'B_Intermediate/pop_data.csv.gz',\n",
    "    permit = set_acceleration['pop_cache'])\n",
    "log_time('RD2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc48382-fdbf-47c5-b7d1-0cbf51cc5691",
   "metadata": {},
   "source": [
    "#### GD3 / RD3 - read and refine text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a2da6a-4fc5-472a-b366-373019afd73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_explanatory_text(addr = 'A_Input/explanation.docx', n = 100):\n",
    "    explain = Document(addr).paragraphs\n",
    "    explain = [txt_wrap(i.text, n) for i in explain]\n",
    "    explain = '\\n'.join(explain)\n",
    "    return explain\n",
    "\n",
    "## execute code\n",
    "explanatory_text = read_explanatory_text()\n",
    "log_time('RD3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13299ba-219b-423d-a3fd-03de2197ceb7",
   "metadata": {},
   "source": [
    "#### GD4 - load CAN/MEX shapefiles for use as masking layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a88293e-e131-435f-b483-3b4a95a5fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_shp(addr):\n",
    "    shp = shapereader.Reader(addr).records()\n",
    "    shp = next(shp).geometry\n",
    "    return ShapelyFeature(shp.geoms, PlateCarree())\n",
    "\n",
    "can_mask   = read_shp('A_Input/gadm40_CAN_shp/gadm40_CAN_0.shp')\n",
    "mex_mask   = read_shp('A_Input/gadm40_MEX_shp/gadm40_MEX_0.shp')\n",
    "usa_border = read_shp('A_Input/gadm40_USA_shp/gadm40_USA_0.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15edf0c1-3995-4c01-9364-0630edb65e14",
   "metadata": {},
   "source": [
    "#### RD4 - Reconcile geographic and population dataset tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9390a35a-4e92-4f56-90c3-adae3c2952f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcile_data(geo = geo_data, pop = pop_data):\n",
    "    pop = pop.loc[geo.index]\n",
    "    return geo, pop\n",
    "\n",
    "##  execute code\n",
    "geo_data, pop_data = reconcile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df65672-a010-40b2-94ac-3906eae80072",
   "metadata": {},
   "source": [
    "# Model Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90542624-5a86-4e97-b5a6-1d3c4845dd75",
   "metadata": {},
   "source": [
    "#### MD1 - Precalculate tract-to-tract distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6fb3cca-b253-4240-8d2f-15f99fa62a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build/Cache Decision: Cache\n"
     ]
    }
   ],
   "source": [
    "## define function to do distance compuations in parallel\n",
    "def measure_distance_in_parallel(geo = geo_data):\n",
    "    \n",
    "    xy = list(zip(geo.INTPTLAT.values, geo.INTPTLON.values))\n",
    "    the_iter = list(range(0, len(xy)))\n",
    "    \n",
    "    ## define engine function that will run on each parallel process\n",
    "    def measure_distance_parallel_slice(n, xy_col = xy):\n",
    "        from geopy.distance import geodesic # for parallel process\n",
    "        xy_col = xy_col.copy()\n",
    "        xy_row = xy_col[n]\n",
    "        xy_dist = []\n",
    "        for i in xy_col[0:n]: xy_dist.append(0)\n",
    "        for i in xy_col[n::]:\n",
    "            xy_dist.append(int(round(geodesic(xy_row, i).miles)))\n",
    "        return xy_dist\n",
    "\n",
    "    ## run engine in parallel for each slice of the data\n",
    "    with Cluster(n = set_acceleration['distance_parallel_cores']) as clust:\n",
    "        view = clust.load_balanced_view()\n",
    "        asyncresult = view.map_async(measure_distance_parallel_slice, the_iter)\n",
    "        asyncresult.wait_interactive()\n",
    "        result = asyncresult.get()\n",
    "        \n",
    "    ## package results and export\n",
    "    result = np.array(result)\n",
    "    result = result + result.T\n",
    "    result = pd.DataFrame(result)\n",
    "    result.index, result.columns = (geo.index, geo.index)\n",
    "    return result\n",
    "\n",
    "## execute code\n",
    "tract_distance = build_or_cache(\n",
    "    function = measure_distance_in_parallel,\n",
    "    address = 'B_Intermediate/tract_distance.csv.gz',\n",
    "    permit = set_acceleration['distance_cache'])\n",
    "log_time('MD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc19f1f2-f863-4872-8427-023b5a662713",
   "metadata": {},
   "source": [
    "#### MD2 - Make one-stage agglomeration clustering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "698a3688-72ee-4f26-8487-ec47ad835e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "## score model\n",
    "def agglom_score(clusters, dist):\n",
    "    \n",
    "    ## load libraries (enables parallel processing)\n",
    "    import numpy as np\n",
    "    \n",
    "    ## construct matrix of points that are in the same group\n",
    "    score = clusters.reshape(clusters.shape[0], 1)\n",
    "    score = (score == score.T).astype(int)\n",
    "    \n",
    "    ## sum distance between points in the same cluster\n",
    "    score = (dist * score).sum().sum()\n",
    "    return score.astype(int)\n",
    "\n",
    "## generate partition of census tracts into k clusters based on proximity\n",
    "def agglom_ml(k, dist, score_func = agglom_score):\n",
    "    \n",
    "    ## load libraries (enables parallel processing)\n",
    "    from sklearn.cluster import AgglomerativeClustering # for parallel process\n",
    "    from numpy           import append\n",
    "    \n",
    "    ## divide census tracts into clusters\n",
    "    ml = AgglomerativeClustering(n_clusters = k,\n",
    "                                 affinity = 'precomputed',\n",
    "                                 linkage = 'average',\n",
    "                                 compute_full_tree = False)\n",
    "    ml_clusters = ml.fit_predict(dist)\n",
    "    \n",
    "    ## score the quality of the cluster solution\n",
    "    ml_score = score_func(ml_clusters, dist)\n",
    "    ml_clusters = append(ml_clusters, ml_score)\n",
    "    \n",
    "    ## export results\n",
    "    return ml_clusters\n",
    "\n",
    "## find best solution (using the fit curve 'elbow' approach)\n",
    "def find_best_cluster_solution(cluster_batch):\n",
    "\n",
    "    ## extract x and y\n",
    "    x = cluster_batch.columns.values\n",
    "    y = cluster_batch.iloc[cluster_batch.shape[0] - 1, :].values\n",
    "    \n",
    "    ## regularize x and y\n",
    "    x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "    y = (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "    x = x.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    ## generate reference x and y\n",
    "    x_ref = np.arange(0, 1, 0.01)\n",
    "    x_ref = x_ref.reshape(-1, 1)\n",
    "    y_ref = 1 - x_ref\n",
    "    x_ref = x_ref\n",
    "    \n",
    "    ## calculate distances between xy and xy_ref\n",
    "    x_dist = (x - x_ref.T)**2\n",
    "    y_dist = (y - y_ref.T)**2\n",
    "    elbow_score = x_dist + y_dist\n",
    "    del x_dist, y_dist, x, y, x_ref, y_ref\n",
    "    \n",
    "    ## find the solution that is furthest from the line of equality (the elbow)\n",
    "    elbow_score = elbow_score.min(axis = 1)\n",
    "    elbow_score = (elbow_score == np.max(elbow_score)).astype(int)\n",
    "    \n",
    "    ## package and export\n",
    "    elbow_score = pd.DataFrame({'Elbow':elbow_score}).T\n",
    "    elbow_score.columns = cluster_batch.columns\n",
    "    cluster_batch = pd.concat([cluster_batch, elbow_score], axis = 0)\n",
    "    \n",
    "    ##  return object\n",
    "    return cluster_batch\n",
    "    \n",
    "## fit clusters in parallel for k = 2 through 50\n",
    "def agglom_batch(k_min = 2, k_max = 48, f = agglom_ml,\n",
    "                 dist_mat = tract_distance,\n",
    "                 cores = set_acceleration['cluster_parallel_cores']):\n",
    "    \n",
    "    ## bound check parameters and construct model list\n",
    "    k_min = max(k_min, 2)\n",
    "    k_max = max(k_min + 1, k_max) + 1\n",
    "    k_range = list(range(k_min, k_max))\n",
    "    \n",
    "    ## set distance matrix default\n",
    "    from functools import partial\n",
    "    f = partial(f, dist = dist_mat)\n",
    "    \n",
    "    ## run models in parallel\n",
    "    with Cluster(n = cores) as clust:\n",
    "        view = clust.load_balanced_view()\n",
    "        asyncresult = view.map_async(f, k_range)\n",
    "        asyncresult.wait_interactive()\n",
    "        result = asyncresult.get()\n",
    "    \n",
    "    ## package results and identity best solution\n",
    "    result = pd.DataFrame(np.array(result).T)\n",
    "    result.columns = k_range\n",
    "    result.index = np.append(dist_mat.index, 'Fit')\n",
    "    result = find_best_cluster_solution(result)\n",
    "    result = pd.DataFrame(result, columns = k_range)\n",
    "    return result\n",
    "\n",
    "## test code\n",
    "cluster_level_one = build_or_cache(\n",
    "    function = agglom_batch,\n",
    "    address = 'B_Intermediate/cluster_level_one.csv.gz',\n",
    "    permit = set_acceleration['cluster_L1_cache'])\n",
    "log_time('MD2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceac255-b803-4fd3-a016-63a5473ef61b",
   "metadata": {},
   "source": [
    "#### MD3 - Conduct two-stage agglomeration clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20f5eba1-fa93-4217-9df6-626a48df66d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "def extract_cluster_solution(cl1):\n",
    "    i = cl1.loc['Elbow', ].astype(bool)\n",
    "    cl1 = cl1.loc[ ~cl1.index.isin(['Fit', 'Elbow']), i].squeeze()\n",
    "    return cl1\n",
    "\n",
    "def agglom_l2(dist = tract_distance, func = agglom_batch,\n",
    "              cl1 = cluster_level_one, ecs = extract_cluster_solution):\n",
    "    \n",
    "    ## extract level one partition and set max number of level two clusters\n",
    "    cl1 = ecs(cl1)\n",
    "    ## max_cl2_clusters = int(48 / max(cl1)) * 2 ## alternative approach\n",
    "    max_cl2_clusters = max(cl1)\n",
    "    \n",
    "    ## generate level two clusters for the cities in each level one cluster\n",
    "    cl2 = dict()\n",
    "    for i in set(cl1):\n",
    "        \n",
    "        ## calculate 2nd level cluster solutions for each cluster\n",
    "        cluster_tracts = cl1[cl1 == i].index\n",
    "        dist_iter = dist.loc[cluster_tracts, cluster_tracts].copy()\n",
    "        k_max_iter = min(max_cl2_clusters, dist.shape[0])\n",
    "        cl2[i] = func(dist_mat = dist_iter, k_max = k_max_iter)\n",
    "        \n",
    "        ## define useful indexes\n",
    "        only_cluster = ~cl2[i].index.isin(['Fit', 'Elbow'])\n",
    "        best_elbow = cl2[i].loc['Elbow'] == 1\n",
    "        \n",
    "        ## extract elbow statistics\n",
    "        #elbow_range = best_elbow.copy()\n",
    "        #elbow_range.iat[0] = True\n",
    "        #elbow_range.iat[elbow_range.shape[0]-1] = True\n",
    "        stats = [cl2[i].loc[~only_cluster, ]]\n",
    "        stat_container = [np.nan for k in range(0, cl2[i].shape[0]-2)]\n",
    "        stat_container[0] = stats\n",
    "\n",
    "        ## extract the best L2 cluster assignments for each L1 cluster\n",
    "        cl2[i] = cl2[i].loc[only_cluster, best_elbow]\n",
    "        cl2[i].columns = ['l2_cluster']\n",
    "        cl2[i]['l1_cluster'] = int(i)\n",
    "        cl2[i]['l2_stats'] = stat_container\n",
    "        \n",
    "    ## consolidate and export data\n",
    "    cl2 = pd.concat(cl2.values(), axis = 0).reset_index()\n",
    "    cl2 = cl2.rename({'index':'GEO_ID'}, axis = 1).set_index('GEO_ID')\n",
    "    return cl2\n",
    "\n",
    "## execute code (Note - stats object is too complex to reassemble correctly.\n",
    "   ## This ok for now as it is only being saved for 'just-in-case' reasons.\n",
    "   ## If needed, it can be converted to a csv-format text string\n",
    "cluster_level_two = build_or_cache(\n",
    "    function = agglom_l2,\n",
    "    address  = 'B_Intermediate/cluster_level_two.csv.gz',\n",
    "    permit   = set_acceleration['cluster_L2_cache']\n",
    "    )\n",
    "geo_data = geo_data.join(cluster_level_two)\n",
    "del cluster_level_two\n",
    "geo_data['l2_index'] = geo_data.l1_cluster.astype(str) +\\\n",
    "    (geo_data.l2_cluster + 65).apply(chr)\n",
    "log_time('MD3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e95783-7f9d-4fc1-b003-793470f42c84",
   "metadata": {},
   "source": [
    "# Calculate Cluster Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e44012-fc80-41ea-b644-49f2d9f81338",
   "metadata": {},
   "source": [
    "#### CSS1 - Aggregate statistics to cluster level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "449db4f6-89a7-49c1-9d00-00e77016f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compile data\n",
    "cluster_data = pop_data.join(geo_data[['INTPTLON', 'INTPTLAT', 'l2_index']])\n",
    "cluster_data = cluster_data.set_index('l2_index')\n",
    "cluster_data = cluster_data.drop(['DP05_0077E'], axis = 1)\n",
    "\n",
    "## transform education into pseudo years of education variable\n",
    "cluster_data['edu_years'] = (cluster_data['DP02_0068E'] * 16)\n",
    "cluster_data['edu_years'] = cluster_data['edu_years'] + (\n",
    "    (cluster_data['DP05_0001E'] - cluster_data['DP02_0068E']) * 12)\n",
    "cluster_data['edu_years'] = (\n",
    "    cluster_data['edu_years'] / cluster_data['DP05_0001E'])\n",
    "\n",
    "## calculate weighted averages and population count sums\n",
    "column_operation = {\n",
    "    'DP02_0068E': 'sum', 'DP03_0088E': 'mean', 'life_expect': 'mean',\n",
    "    'rep_vote': 'sum', 'total_vote': 'sum', 'edu_years': 'mean',\n",
    "    'INTPTLON': 'mean', 'INTPTLAT': 'mean'}\n",
    "\n",
    "for i in column_operation.keys():\n",
    "    if column_operation[i] == 'mean':\n",
    "        cluster_data[i] = cluster_data[i] * cluster_data['DP05_0001E']\n",
    "        \n",
    "cluster_data = cluster_data.groupby('l2_index').sum()\n",
    "\n",
    "for i in column_operation.keys():\n",
    "    if column_operation[i] == 'mean':\n",
    "        cluster_data[i] = cluster_data[i] / cluster_data['DP05_0001E']\n",
    "\n",
    "        \n",
    "## inflate counts to compensate for sampling\n",
    "for i in column_operation.keys():\n",
    "    if column_operation[i] == 'sum':\n",
    "        cluster_data[i] = cluster_data[i] / set_acceleration['sample_size']\n",
    "cluster_data['DP05_0001E'] = ((cluster_data['DP05_0001E']\n",
    "                             ) / set_acceleration['sample_size']).astype(int)\n",
    "\n",
    "## scale to hdi factors\n",
    "cluster_data['hdi_edu'] = ((cluster_data['edu_years'] / 15) + (\n",
    "    cluster_data['edu_years'] / 18)) / 2\n",
    "cluster_data['hdi_inc'] = (np.log(cluster_data['DP03_0088E']) - np.log(100)\n",
    "    ) / (np.log(75e3) - np.log(100))\n",
    "cluster_data['hdi_lif'] = (cluster_data['life_expect'] - 20) / (85 - 20)\n",
    "\n",
    "## calculate hdi\n",
    "cluster_data['hdi'] = (\n",
    "    cluster_data['hdi_edu'] * cluster_data['hdi_inc'] * cluster_data['hdi_edu'])\n",
    "cluster_data['hdi'] = ((cluster_data['hdi'] ** (1/3)) * 1000).astype(int)\n",
    "cluster_data['hdi_label'] = cluster_data['hdi'].astype(str)\n",
    "\n",
    "## clean up columns\n",
    "cluster_data = cluster_data.rename({\n",
    "    'DP02_0068E':'education', 'DP03_0088E':'income',\n",
    "    'DP05_0001E':'population'}, axis = 1)\n",
    "cluster_data = cluster_data.drop(\n",
    "    ['hdi_edu', 'hdi_inc', 'hdi_lif', 'education'], axis = 1)\n",
    "cluster_data['income'] = cluster_data['income'].round(0).astype(int)\n",
    "\n",
    "## \n",
    "cluster_data['rep_pct'] = cluster_data['rep_vote'] / cluster_data['total_vote']\n",
    "cluster_data['log_pop'] = np.log(cluster_data['population'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa13a9-d4ec-40c1-840c-9d3ca9a2fe2c",
   "metadata": {},
   "source": [
    "#### CSS2 - Fill in explanation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6b8c89e-b560-4ff9-a7c1-a2ec9bec493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_explain_stats(txt = explanatory_text, dat = cluster_data):\n",
    "    \n",
    "    ## make shell to hold statistics\n",
    "    stats = dict()\n",
    "    \n",
    "    ## calculate number of new states\n",
    "    stats['state_count'] = dat.shape[0]\n",
    "    \n",
    "    ## median scores\n",
    "    stats['hdi_median'] = dat.hdi.median().astype(int)\n",
    "    stats['pop_median'] = (dat.population.median()/ 1e6).round(1)\n",
    "    stats['vote_median'] = (dat.rep_pct.median() * 1e2).round(1).astype(int)\n",
    "    \n",
    "    ## Figure 3 statistics\n",
    "    x = dat.copy().population.sort_values(ascending = False)\n",
    "    stats['pop_top_sum'] = 0\n",
    "    stats['pop_top_count'] = 0\n",
    "    for i in x.index:\n",
    "        stats['pop_top_sum'] += x[i]\n",
    "        stats['pop_top_count'] += 1\n",
    "        if stats['pop_top_sum'] > (x.sum() * (1/2)): break\n",
    "    stats['pop_other_count'] = stats['state_count'] - stats['pop_top_count']\n",
    "    stats['pop_top_sum'] = (stats['pop_top_sum'] / 1e6).round(1)\n",
    "    del x\n",
    "    \n",
    "    ## Figure 4 statistics - politics\n",
    "    x = dat.copy().rep_pct.sort_values(ascending = False) - 0.5\n",
    "    stats['rep_count'] = (x >=  0.05).sum()\n",
    "    stats['dem_count'] = (x <= -0.05).sum()\n",
    "   \n",
    "    \n",
    "    \n",
    "    ## Figure 4 statistics - politics x hdi\n",
    "    x = pd.concat([x, dat['hdi']], axis = 1)\n",
    "    x['rep'] = pd.cut(x.rep_pct, [-9999, -0.05, 0.05, 9999],\n",
    "                  labels = ['dem', 'swing', 'rep'])\n",
    "    x = x.groupby('rep').median().round(0).astype(int)\n",
    "    stats['rep_lean_hdi'] = x.loc['rep', 'hdi']\n",
    "    stats['dem_lean_hdi'] = x.loc['dem', 'hdi']\n",
    "    \n",
    "    ## insert statistics into the text\n",
    "    return txt.format(**stats)\n",
    "\n",
    "## execute code\n",
    "explanatory_text = calculate_explain_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127f950-ff08-4315-a527-81ad7d265452",
   "metadata": {},
   "source": [
    "# Prepare Visualization Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7399d4c-7b28-4412-a896-4955b2b66b0e",
   "metadata": {},
   "source": [
    "#### PVD1 - Generate Tract Centroid Voronoi Decomp. Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1412ba81-f5bc-4cdc-87e1-fa0a7e6e750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_voronoi(xy = geo_data[['INTPTLON', 'INTPTLAT']]):\n",
    "    \n",
    "    ## calculate voronoi decomposition\n",
    "    vor_obj = Voronoi(np.array(xy))\n",
    "    \n",
    "    ## assemble polygons for each point\n",
    "    vor_polys = []\n",
    "    for i in vor_obj.point_region:\n",
    "        j = [x for x in vor_obj.regions[i] if x >= 0]\n",
    "        vor_polys.append(vor_obj.vertices[j, :])\n",
    "            \n",
    "    ## generate valid polygons and return object\n",
    "    for i in range(0, len(vor_polys)):\n",
    "        try: vor_polys[i] = make_valid(Polygon(vor_polys[i]))\n",
    "        except: vor_polys[i] = None\n",
    "    return vor_polys\n",
    "        \n",
    "\n",
    "## execute code\n",
    "tract_polys = calculate_voronoi()\n",
    "log_time('PVD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179d05e-e7e3-40b3-8fd4-65514612a7cf",
   "metadata": {},
   "source": [
    "#### PVD2 - Generate Cluster Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c4676da-dc84-42b5-a6d8-4c62a5f6ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_polys(polys, groups):\n",
    "\n",
    "    ## sort polygons into cluster groups\n",
    "    cluster_poly = dict()\n",
    "    for i in set(groups.values): cluster_poly[i] = []\n",
    "    for i in range(0, len(polys) - 1):\n",
    "        group_name = groups.iat[i]\n",
    "        if polys[i] is not None:\n",
    "            cluster_poly[group_name].append(polys[i])\n",
    "\n",
    "    ## merge cluster group polygons and return\n",
    "    for i in cluster_poly.keys():\n",
    "        cluster_poly[i] = unary_union(cluster_poly[i])\n",
    "\n",
    "    return cluster_poly\n",
    "\n",
    "## execute code\n",
    "clust_polys_l1 = merge_polys(tract_polys, geo_data.l1_cluster)\n",
    "clust_polys_l2 = merge_polys(tract_polys, geo_data.l2_index)\n",
    "log_time('PVD2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68dbd4-26b4-4ee4-8d7b-6ddeb8ea3371",
   "metadata": {},
   "source": [
    "# Render Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a6167-aea1-47fa-89a3-3a3405d9fb0c",
   "metadata": {},
   "source": [
    "#### RV0 - make the basic infrastructure for the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16ac184b-0922-4b57-b5f8-742e0a0763cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "def draw_plot_foundation():\n",
    "    \n",
    "    ## make figure and grid objects\n",
    "    global poster_fig\n",
    "    poster_fig = plt.figure(figsize = (36, 24))\n",
    "    poster_grid  = poster_fig.add_gridspec(12, 18, figure = poster_fig,\n",
    "                                        hspace = 0.01, wspace = 0.01, top = 1,\n",
    "                                        left = 0, right = 1, bottom = 0)\n",
    "    poster_fig.set_facecolor(set_color['OrangeDark'])\n",
    "    \n",
    "    ## --\n",
    "    \n",
    "    ## define axes\n",
    "    global poster_ax\n",
    "    poster_ax = dict()\n",
    "    \n",
    "    ## main map (6x9)\n",
    "    poster_ax['map1'] = poster_fig.add_subplot(\n",
    "        poster_grid[0:6, 0:9], projection = set_map['map_proj'])\n",
    "\n",
    "    \n",
    "    ## explanation panel (6x9)\n",
    "    poster_ax['explain'] = poster_fig.add_subplot(poster_grid[6:12, 0:9])\n",
    "\n",
    "    ## 2x1 bar charts (6x3)\n",
    "    poster_ax['bar1'] = poster_fig.add_subplot(poster_grid[0:6, 9:12])\n",
    "    poster_ax['bar2'] = poster_fig.add_subplot(poster_grid[0:6, 12:15])\n",
    "    poster_ax['bar3'] = poster_fig.add_subplot(poster_grid[0:6, 15:18])\n",
    "    \n",
    "    ## 1x2 mini-maps (3x3)\n",
    "    poster_ax['minimap1'] = poster_fig.add_subplot(\n",
    "        poster_grid[6:8, 9:12], projection = set_map['map_proj'])\n",
    "    poster_ax['minimap2'] = poster_fig.add_subplot(\n",
    "        poster_grid[6:8, 12:15], projection = set_map['map_proj'])\n",
    "    poster_ax['minimap3'] = poster_fig.add_subplot(\n",
    "        poster_grid[6:8, 15:18], projection = set_map['map_proj'])\n",
    "    \n",
    "    ## methods\n",
    "    poster_ax['elbow'] = poster_fig.add_subplot(poster_grid[8:10, 9:12])\n",
    "    poster_ax['map2'] = poster_fig.add_subplot(\n",
    "        poster_grid[10:12, 9:12], projection = set_map['map_proj'])\n",
    "    poster_ax['method'] = poster_fig.add_subplot(poster_grid[8:12, 12:18])\n",
    "\n",
    "    ## remove axis ticks\n",
    "    for i in poster_ax.keys():\n",
    "        poster_ax[i].tick_params(\n",
    "            bottom = False, top = False, left = False, right = False,\n",
    "            labelbottom = False, labeltop = False, labelleft = False,\n",
    "            labelright = False, color = 'red')\n",
    "        poster_ax[i].set_facecolor(set_color['OrangeBG'])\n",
    "        \n",
    "## execute code\n",
    "draw_plot_foundation()\n",
    "log_time('RV0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdb9ab-81c4-41c6-8374-624cad481852",
   "metadata": {},
   "source": [
    "#### RV1 - draw explanation panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c871d19a-6a5b-4561-adad-1df5fa151c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_explanation(txt = explanatory_text):\n",
    "    poster_ax['explain'].set_ylim(0, 6)\n",
    "    poster_ax['explain'].set_xlim(0, 9)\n",
    "    poster_ax['explain'].text(x = 9 * 0.03, y = 6 * 0.03, s = txt,\n",
    "        fontweight = 'bold',\n",
    "        horizontalalignment = 'left', verticalalignment = 'bottom',\n",
    "        fontsize = set_font['medium'], color = set_color['OrangeDark'])\n",
    "    \n",
    "render_explanation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1600935-4d81-4809-b2aa-21fa53b1da34",
   "metadata": {},
   "source": [
    "#### RV2 - draw cluster map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65311cb6-7c5a-4d9b-8960-47244f43d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- layers\n",
    "## 0 - States\n",
    "## 1 - Lakes\n",
    "## 3 - Tract centroids\n",
    "## 5, 6 - Cluster boundaries\n",
    "## 8 - Ocean\n",
    "## 9 - Cluster labels\n",
    "\n",
    "## draw generic map layers\n",
    "def draw_generic_map(ax, map_mask = [can_mask, mex_mask]):\n",
    "    poster_ax[ax].set_extent(set_map['bounds'])\n",
    "    poster_ax[ax].add_geometries(list(feature.STATES.geometries()),\n",
    "        facecolor = '#33221100', edgecolor = set_color['OrangeLight'],\n",
    "        lw = 1, zorder = 0, crs = PlateCarree())\n",
    "    poster_ax[ax].add_feature(feature.LAKES,\n",
    "        facecolor = set_color['OrangeBG'], edgecolor = set_color['OrangeLight'],\n",
    "        lw = 1, zorder = 1)\n",
    "    for i in map_mask:\n",
    "        poster_ax[ax].add_feature(i, facecolor = set_color['OrangeBG'],\n",
    "            edgecolor = set_color['OrangeLight'], lw = 1, zorder = 7)\n",
    "    poster_ax[ax].add_feature(feature.OCEAN,\n",
    "        facecolor = set_color['OrangeBG'], edgecolor = set_color['OrangeLight'],\n",
    "        lw = 1, zorder = 8)\n",
    "    \n",
    "## draw census tract centroids\n",
    "def draw_census_centroids(ax, geo = geo_data):\n",
    "    poster_ax[ax].scatter(\n",
    "        x = geo_data.INTPTLON.values, y = geo_data.INTPTLAT.values,\n",
    "        color = set_color['OrangeMedium'], zorder = 3,\n",
    "        transform = PlateCarree(), s = 1)\n",
    "    \n",
    "## label clusters\n",
    "def label_cluster(ax, label_col = None, dat = cluster_data):\n",
    "\n",
    "    if label_col is None: dat['label'] = dat.index\n",
    "    else: dat['label'] = dat.index + '\\n' + dat[label_col]\n",
    "\n",
    "    bb = dict(edgecolor = '#33221100', lw = 3,\n",
    "              facecolor = set_color['AzureBG'] + '80', boxstyle = 'round')\n",
    "\n",
    "    for i in dat.index:\n",
    "        poster_ax[ax].text(\n",
    "            x = dat.at[i, 'INTPTLON'], y = dat.at[i, 'INTPTLAT'],\n",
    "            s = dat.at[i, 'label'],\n",
    "            fontsize = set_font['small'], color = set_color['AzureDark'],\n",
    "            transform = PlateCarree(), fontweight = 'bold',\n",
    "            bbox = bb, zorder = 9)\n",
    "\n",
    "## draw cluster boundaries\n",
    "def draw_cluster(ax, poly, the_lw = 2, zo = 5):\n",
    "    for i in poly.keys():\n",
    "        poster_ax[ax].plot(*poly[i].exterior.xy,\n",
    "            zorder = zo, color = set_color['AzureBG'],\n",
    "            transform = PlateCarree(), lw = the_lw * 1.5)\n",
    "        poster_ax[ax].plot(*poly[i].exterior.xy,\n",
    "            zorder = zo + 1, color = set_color['AzureDark'],\n",
    "            transform = PlateCarree(), lw = the_lw)\n",
    "\n",
    "## execute code - map 1\n",
    "draw_generic_map('map1')\n",
    "draw_census_centroids('map1')\n",
    "draw_cluster('map1', clust_polys_l2)\n",
    "label_cluster('map1')\n",
    "\n",
    "## execute code - map 1\n",
    "draw_generic_map('map2')\n",
    "draw_census_centroids('map2')\n",
    "draw_cluster('map2', clust_polys_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf58ce-b8e8-474f-96ef-e7e3c5937f6f",
   "metadata": {},
   "source": [
    "#### RV3 - bar panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66c2d825-5009-40c7-8cb3-173d4bf3bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bar(ax, major_col, minor_col = None, dat = cluster_data.copy(),\n",
    "             unit = None):\n",
    "    \n",
    "    dat = dat.sort_values('hdi').copy()\n",
    "    \n",
    "    ## scale data for visualization purposes\n",
    "    def min_max_scale(x):\n",
    "        x = (x - min(x)) / (max(x) - min(x))\n",
    "        n = 0.99\n",
    "        x = (x * n) + (1 - n)\n",
    "        return x\n",
    "    \n",
    "    dat['scaled_major'] = min_max_scale(dat[major_col])\n",
    "        \n",
    "    ## convert bar labels\n",
    "    dat['label_major'] = dat[major_col]\n",
    "    if unit == '/M':\n",
    "        dat['label_major'] = (dat['label_major'] / 1e6).round(1)\n",
    "        dat['label_major'] = dat['label_major'].astype(str) + 'M'\n",
    "        min_display_major = 0\n",
    "    elif unit == '%':\n",
    "        dat['label_major'] = (dat['label_major'] * 1e2).round(0).astype(int)\n",
    "        dat['label_major'] = dat['label_major'].astype(str) + '%'\n",
    "        min_display_major = 0\n",
    "    else:\n",
    "        min_display_major = 0\n",
    "          \n",
    "    ## calculate display parameters\n",
    "    bar_ceiling = 0.75\n",
    "    bar_start = 0.10\n",
    "    lab_pad = 0.02\n",
    "    poster_ax[ax].set_xlim(0, 1)\n",
    "    poster_ax[ax].set_ylim(0, 2)\n",
    "    bar_space = 1.9 / (dat.shape[0] + 1)\n",
    "    dat['bar_spacing'] = np.arange(bar_space, 1.9, bar_space)\n",
    "    bb_blue = dict(edgecolor = set_color['AzureDark'], lw = 2,\n",
    "             facecolor = set_color['AzureBG'] + '80', boxstyle = 'square')\n",
    "    bb_orange = dict(edgecolor = set_color['OrangeMedium'], lw = 2,\n",
    "             facecolor = set_color['OrangeLight'], boxstyle = 'square')\n",
    "\n",
    "    ## draw major bar\n",
    "    poster_ax[ax].barh(\n",
    "        y = dat['bar_spacing'], height = bar_space,\n",
    "        width = dat['scaled_major'] * bar_ceiling,\n",
    "        color = set_color['OrangeLight'], edgecolor = set_color['OrangeDark'],\n",
    "        lw = 2, left = bar_start)\n",
    "    \n",
    "    ## draw bar labels\n",
    "    for i in dat.index:\n",
    "        poster_ax[ax].text(\n",
    "            y = dat.loc[i, 'bar_spacing'], x = lab_pad, s = i,\n",
    "            horizontalalignment = 'left', verticalalignment = 'center',\n",
    "            fontsize = set_font['small'], fontweight = 'bold',\n",
    "            color = set_color['OrangeDark'])\n",
    "        \n",
    "    ## label major bars\n",
    "    for i in dat.index:\n",
    "        if dat.loc[i, major_col] > min_display_major:\n",
    "            poster_ax[ax].text(\n",
    "                y = dat.loc[i, 'bar_spacing'],\n",
    "                x = bar_start + dat.loc[i, 'scaled_major'] * (\n",
    "                    bar_ceiling + 0.01),\n",
    "                s = dat.loc[i, 'label_major'],\n",
    "                horizontalalignment = 'left', verticalalignment = 'center',\n",
    "                fontsize = set_font['small'], fontweight = 'bold',\n",
    "                color = set_color['OrangeDark'])\n",
    "\n",
    "## execute code\n",
    "draw_bar('bar1', 'hdi')\n",
    "draw_bar('bar2', 'population',unit = '/M')\n",
    "draw_bar('bar3', 'rep_pct', unit = '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77caf8-a155-415f-996b-c995e5e4cf23",
   "metadata": {},
   "source": [
    "#### RV4 - Simplified Color Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "902e7bd8-d1df-416a-82a5-3c52df3fbf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brew_colors(stat_series, stat_cuts):\n",
    "    stat_cuts = np.append(stat_cuts.values, 9e9)\n",
    "    stat_cuts = np.append(-9e9, stat_cuts)\n",
    "    stat_labels = ['AzureBG', 'AzureLight', 'AzureMedium']\n",
    "    stat_series = pd.cut(stat_series, stat_cuts, labels = stat_labels)\n",
    "    stat_series = stat_series.replace(set_color)\n",
    "    return stat_series\n",
    "\n",
    "def draw_color_cluster(ax, poly_color = None,\n",
    "        poly = clust_polys_l2, map_mask = [can_mask, mex_mask]):\n",
    "\n",
    "    poster_ax[ax].set_extent(set_map['bounds'])\n",
    "    \n",
    "    if poly_color is None:\n",
    "        temp = dict()\n",
    "        for i in poly.keys():\n",
    "            temp[i] = '#FF0044'\n",
    "        poly_color = temp\n",
    "\n",
    "    for i in poly.keys():\n",
    "        poster_ax[ax].fill(*poly[i].exterior.xy,\n",
    "            zorder = 0, edgecolor = set_color['OrangeMedium'],\n",
    "            facecolor = poly_color[i],\n",
    "            transform = PlateCarree(), lw = 2)\n",
    "\n",
    "    for i in map_mask:\n",
    "        poster_ax[ax].add_feature(i, facecolor = set_color['OrangeBG'],\n",
    "            edgecolor = set_color['OrangeLight'], lw = 1, zorder = 8)\n",
    "    poster_ax[ax].add_feature(feature.OCEAN,\n",
    "        facecolor = set_color['OrangeBG'], edgecolor = set_color['OrangeLight'],\n",
    "        lw = 1, zorder = 9)\n",
    "\n",
    "## execute code - color maps\n",
    "new_state_colors = brew_colors(cluster_data['hdi'],\n",
    "                         cluster_data.hdi.quantile([0.25, 0.75]))\n",
    "draw_color_cluster('minimap1', new_state_colors)\n",
    "\n",
    "new_state_colors = brew_colors(cluster_data['population'],\n",
    "                         cluster_data.population.quantile([0.25, 0.75]))\n",
    "draw_color_cluster('minimap2', new_state_colors)\n",
    "\n",
    "new_state_colors = brew_colors(cluster_data['rep_pct'],\n",
    "                         cluster_data.rep_pct.quantile([0.25, 0.75]))\n",
    "draw_color_cluster('minimap3', new_state_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b3bed7-8d9a-43d6-a71d-091da66a5140",
   "metadata": {},
   "source": [
    "#### RV5 - make titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82bf0029-1941-481e-90af-181cfd29ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_title(ax, title):\n",
    "    margin = max(poster_ax[ax].get_xlim()) * 0.01\n",
    "    poster_ax[ax].text(\n",
    "        x = max(poster_ax[ax].get_xlim()) - margin,\n",
    "        y = max(poster_ax[ax].get_ylim()) - margin,\n",
    "        s = title,\n",
    "        fontsize = set_font['medium'], fontweight = 'bold',\n",
    "        color = set_color['OrangeDark'], zorder = 11,\n",
    "        horizontalalignment = 'right', verticalalignment = 'top')\n",
    "\n",
    "## park titles here\n",
    "make_title('map1', 'Figure 1: How State Borders Might Look If They ' +\\\n",
    "           'Corresponded To Where People Live')\n",
    "\n",
    "make_title('bar1', 'Fig. 2a: HDI Score For Each Cluster')\n",
    "make_title('bar2', 'Fig. 3a: Population Total For\\nEach Cluster')\n",
    "make_title('bar3', 'Fig. 4a: Percent Voting For The\\nRepublican ' +\\\n",
    "           'Pres. Candidate in 2020')\n",
    "\n",
    "make_title('minimap1', 'Figure 2b: HDI Quantile Map')\n",
    "make_title('minimap2', 'Figure 3b: Population Quantile Map')\n",
    "make_title('minimap3', 'Figure 4b: Republican Quantile Map')\n",
    "\n",
    "make_title('method','Method: Calculating New State Borders')\n",
    "make_title('elbow','Figure 5:')\n",
    "make_title('map2', 'Figure 6:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1905627-f745-4265-8fe0-133a3985922f",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df9eb9-cef0-40a2-9700-f3b77a68de61",
   "metadata": {},
   "source": [
    "#### Test 1 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff17c8-1c3b-45f5-9a53-f007101ac066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7732abb0-33ea-45b3-8efa-a4f03b65e63c",
   "metadata": {},
   "source": [
    "#### Test 2 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c8a2a9-0dd7-47da-a1ef-f253a6e9018d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2072d112-7ed8-4bdb-be59-d11f11b05828",
   "metadata": {},
   "source": [
    "#### Test 3 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee7cfd-e870-484c-85f9-32e2280adc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46b648d2-2c43-48a9-9520-60482c4f46fd",
   "metadata": {},
   "source": [
    "# Footer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe42104-9eb2-47be-8050-1f1d1125eede",
   "metadata": {},
   "source": [
    "#### write poster to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2860728-2bc2-401e-a043-c037c8752604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_poster():\n",
    "    poster_fig.savefig('C_Output/pop_cluster_map.png')\n",
    "    poster_fig.savefig('C_Output/pop_cluster_map.pdf')\n",
    "save_poster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "257b6438-6ef8-4ea2-b4d8-a5039053e607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time log:\n",
      "   H2: 21:16:29\n",
      "   H3: 21:16:29\n",
      "  GD1: 21:16:29\n",
      "  RD1: 21:16:29\n",
      "  RD2: 21:16:29\n",
      "  RD3: 21:16:29\n",
      "  MD1: 21:17:34\n",
      "  MD2: 21:17:34\n",
      "  MD3: 21:17:34\n",
      " PVD1: 21:17:35\n",
      " PVD2: 21:17:36\n",
      "  RV0: 21:17:36\n",
      "  End: 21:19:14\n"
     ]
    }
   ],
   "source": [
    "log_time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
