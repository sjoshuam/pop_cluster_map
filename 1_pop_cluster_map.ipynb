{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54ca8b13-5c8a-41df-b762-d948d0e36e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84ff307-1f5b-4341-9bd9-90d82b9a60b1",
   "metadata": {},
   "source": [
    "# H - Header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2fd9fc-433c-48fe-a986-f74fb1c692e4",
   "metadata": {},
   "source": [
    "#### H1 - Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2893cbc5-099e-469b-88d6-3481d734115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard foundational libraries\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## import specific functions\n",
    "from os                 import mkdir, listdir\n",
    "from os.path            import isfile, isdir\n",
    "from datetime           import datetime, timedelta\n",
    "from cartopy            import feature\n",
    "from cartopy.crs        import LambertConformal, PlateCarree\n",
    "from dbfread            import DBF\n",
    "from docx               import Document\n",
    "from textwrap           import fill as txt_wrap\n",
    "from geopy.distance     import geodesic\n",
    "from ipyparallel        import Cluster\n",
    "from sklearn.cluster    import AgglomerativeClustering\n",
    "from functools          import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e46a131-c865-43fc-80be-1c2c1576466f",
   "metadata": {},
   "source": [
    "#### H2 - Basic Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e061a3d-8cb6-4bd5-8f67-881c337b1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up standard directories if needed\n",
    "def make_standard_file_system():\n",
    "    for i in ['A_Input', 'B_Intermediate', 'C_Output']:\n",
    "        if not isdir(i): mkdir(i)\n",
    "\n",
    "## log time elapsed\n",
    "time_log = dict()\n",
    "def log_time(the_id = 'End Log'):\n",
    "    \n",
    "    ## construct new time stamp\n",
    "    now_time = str(datetime.now().hour).zfill(2)\n",
    "    now_time = now_time +':'+ str(datetime.now().minute).zfill(2)\n",
    "    now_time = now_time +':'+ str(datetime.now().second).zfill(2)\n",
    "\n",
    "    ## add to time log\n",
    "    if the_id == 'End Log':\n",
    "        time_log['End'] = now_time\n",
    "        print('Time log:')\n",
    "        for i in time_log.keys():\n",
    "            print(i.rjust(5) + ':', time_log[i])\n",
    "    else:\n",
    "        time_log[the_id] = now_time\n",
    "        \n",
    "## toggle cache versus build\n",
    "def build_or_cache(function, address, permit):\n",
    "    if permit and isfile(address):\n",
    "        print('Build/Cache Decision: Cache')\n",
    "        the_file = pd.read_csv(address, index_col = 0)\n",
    "    else:\n",
    "        print('Build/Cache Decision: Build')\n",
    "        the_file = function()\n",
    "        the_file.to_csv(address)\n",
    "    return the_file\n",
    "    \n",
    "## execute functions\n",
    "make_standard_file_system()\n",
    "log_time('H2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12f165-5257-40f4-b625-5950dac2d955",
   "metadata": {},
   "source": [
    "#### H3 - Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9577a3a0-6eb6-4a48-a510-a8d779269232",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set color palette\n",
    "set_color = {\n",
    "    'AzureDark'    :(7/12, 1.0, 0.4),\n",
    "    'AzureMedium'  :(7/12, 0.7, 0.7),\n",
    "    'AzureLight'   :(7/12, 0.4, 1.0),\n",
    "    'AzureBG'      :(7/12, 0.1, 1.0),\n",
    "    'AzureOverlay' :(7/12, 0.1, 1.0, 0.5),\n",
    "    \n",
    "    'OrangeDark'   :(1/12, 1.0, 0.4),\n",
    "    'OrangeMedium' :(1/12, 0.7, 0.7),\n",
    "    'OrangeLight'  :(1/12, 0.4, 1.0),\n",
    "    'OrangeBG'     :(1/12, 0.1, 1.0),\n",
    "    'OrangeOverlay':(1/12, 0.1, 1.0, 0.5) \n",
    "    }\n",
    "\n",
    "## set font sizes\n",
    "set_font = {\n",
    "    'small' : 16,\n",
    "    'medium': 24,\n",
    "    'large' : 32\n",
    "    }\n",
    "\n",
    "## time-saver settings\n",
    "set_acceleration = {\n",
    "    'dbf_cache'             : True,\n",
    "    'sample_size'           : 1/16,\n",
    "    'pop_cache'             : True,\n",
    "    'distance_cache'        : True,\n",
    "    'cluster_parallel_cores': 6,\n",
    "    'cluster_L1_cache'      : True,\n",
    "    'cluster_L2_cache'      : True\n",
    "    }\n",
    "\n",
    "## map parameters\n",
    "set_map = {\n",
    "    'bounds'  : [-124.73 + 5, -66.95 - 5, 25.12 - 3.3, 49.38 + 3.3],\n",
    "    'map_proj': LambertConformal(\n",
    "        central_longitude = (-124.73 - 66.95) / 2,\n",
    "        central_latitude = (25.12 + 49.38) / 2,\n",
    "        standard_parallels = (25.12, 49.38))\n",
    "    }\n",
    "log_time('H3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255532f-66e3-45f4-b62c-aaf8f2941744",
   "metadata": {},
   "source": [
    "# GD - Gather Data / RD - Refine Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149febf-9577-46d1-80f9-aa0a37dad8f9",
   "metadata": {},
   "source": [
    "#### GD1 - read census tract geographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b030f7-3b12-41e6-9dc5-1fe7eabbf20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build/Cache Decision: Build\n"
     ]
    }
   ],
   "source": [
    "def read_geo_data(directory = 'A_Input/tracts_dbf'):\n",
    "    \n",
    "    ## list dbf files in target directory\n",
    "    dbf_addr = listdir(directory)\n",
    "    dbf_addr = [i for i in dbf_addr if i[-3::] == 'dbf']\n",
    "    \n",
    "    ## define relevant columns from each\n",
    "    desired_columns = {'GEOID': str,\n",
    "                       'STATEFP': str, 'COUNTYFP':str, 'TRACTCE':str,\n",
    "                        'INTPTLAT': float, 'INTPTLON': float, 'ALAND': int}\n",
    "    \n",
    "    ## read in all dbf files\n",
    "    dbf_data = []\n",
    "    for i in dbf_addr:\n",
    "        i_dbf = pd.DataFrame(iter(DBF(directory + '/' + i)))\n",
    "        i_dbf = i_dbf[desired_columns.keys()].astype(desired_columns)\n",
    "        dbf_data.append(i_dbf)\n",
    "        \n",
    "    ## compile data into a single file\n",
    "    dbf_data = pd.concat(dbf_data, axis = 0).sort_values('GEOID')\n",
    "    dbf_data['count'] = 1\n",
    "    dbf_data = dbf_data.reset_index(drop = True)\n",
    "    \n",
    "    ## repair GEOID irregularities\n",
    "    dbf_data['GEOID'] = dbf_data['GEOID'].astype(str).str.zfill(11)\n",
    "    \n",
    "    ## export data\n",
    "    return dbf_data\n",
    "\n",
    "## execute code\n",
    "geo_data = build_or_cache(function = read_geo_data,\n",
    "                          address = 'B_Intermediate/dbf_data.csv.gz',\n",
    "                          permit = set_acceleration['dbf_cache'])\n",
    "log_time('GD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b44dd8-b3a4-4eff-a644-21bb455d388a",
   "metadata": {},
   "source": [
    "#### RD1 - draw a sample from the census tract geographic data and exclude outlier tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3fd248c-6dab-40fe-83a5-2228ac349e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_geo_data(dat = geo_data, too_rural = 20.720e6 * 5,\n",
    "                    too_much = set_acceleration['sample_size']):\n",
    "    \n",
    "    ## filter out extremely rural areas (< 100 people per square mile)\n",
    "    dat = dat.loc[dat.ALAND < too_rural, :]\n",
    "    \n",
    "    ## take systematic sample of the data\n",
    "    i = np.arange(0, dat.shape[0]) % int(1 / too_much)\n",
    "    dat = dat.loc[i == 0, ]\n",
    "    \n",
    "    ## set index\n",
    "    dat = dat.rename({'GEOID':'GEO_ID'}, axis = 1).set_index('GEO_ID')\n",
    "    dat.index = 'ct' + dat.index.astype(str).str.zfill(11)\n",
    "\n",
    "    ## return data\n",
    "    return dat\n",
    "\n",
    "## execute code\n",
    "geo_data = refine_geo_data()\n",
    "log_time('RD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3373db0-4b46-4aac-8b01-9e6d8dfd92da",
   "metadata": {},
   "source": [
    "#### GD2 - read population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28942f36-b9af-41e1-a844-c489d8d20c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pop_data(roster = 'A_Input/sources.csv'):\n",
    "    \n",
    "    ## read in data file roster\n",
    "    roster = pd.read_csv(roster).set_index('OBJ_NAME')\n",
    "    \n",
    "    ## load all datasets in the roster file\n",
    "    pop_data = dict()\n",
    "    for i in roster.index:\n",
    "        var_names = roster.loc[i, 'VAR_NAME'].split(';')\n",
    "        pop_data[i] = pd.read_csv('A_Input' + '/' + roster.loc[i, 'FILE_NAME'],\n",
    "            usecols = var_names, dtype = str)\n",
    "        \n",
    "    ## merge census datasets\n",
    "    census_i = roster.index[roster.SOURCE == 'data.census.gov'].values\n",
    "    census_dat_count = 0\n",
    "    \n",
    "    for i in census_i:\n",
    "        pop_data[i] = pop_data[i].loc[1::, :].set_index('GEO_ID')\n",
    "        if census_dat_count < 1:\n",
    "            pop_data['census'] = pop_data[i]\n",
    "            pop_data.pop(i)\n",
    "            census_dat_count += 1\n",
    "        else:\n",
    "            pop_data['census'] = pop_data['census'].join(pop_data[i])\n",
    "            pop_data.pop(i)\n",
    "            census_dat_count += 1\n",
    "            \n",
    "    ## convert census data to numeric\n",
    "    def robust_int(x):\n",
    "        try: x = int(x)\n",
    "        except: x = 0\n",
    "        return x\n",
    "    map_robust_int = lambda x: x.map(robust_int)\n",
    "    pop_data['census'] = pop_data['census'].apply(map_robust_int)\n",
    "\n",
    "    return pop_data\n",
    "\n",
    "## execute code - see RD2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd744f8f-2948-4bc1-b8f3-61a41011a660",
   "metadata": {},
   "source": [
    "#### RD2 - refine and compile population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba287d1-300b-42d3-a930-6865bcfab541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build/Cache Decision: Build\n"
     ]
    }
   ],
   "source": [
    "def refine_pop_data(pop):\n",
    "    \n",
    "    ## -- standardize geographic codes as needed\n",
    "    state_fips = pop.pop('state_fips')# if needed in future; not currently used\n",
    "    pop['census'].index = pop['census'].index.str.replace('1400000US', '')\n",
    "\n",
    "    ## -- refine life expectancy data and merge into census data\n",
    "    pop['lifespan'].columns = ['GEO_ID', 'life_expect']\n",
    "    pop['lifespan'].life_expect = pop['lifespan'].life_expect.astype(float)\n",
    "    pop['census'] = pop['census'].join(pop['lifespan'].set_index('GEO_ID'))\n",
    "    pop.pop('lifespan')\n",
    "    \n",
    "    ## -- refine voting data\n",
    "    \n",
    "    ## filter to necessary data\n",
    "    i = (pop['vote'].party == 'REPUBLICAN') & (pop['vote'].year == '2020')\n",
    "    pop['vote'] = pop['vote'].loc[i].drop(['year', 'party', 'state_po'],\n",
    "                                          axis = 1)\n",
    "    \n",
    "    ## impute total votes (data is irregular from state to state)\n",
    "    temp = pop['vote'].copy()\n",
    "    temp = temp.drop(['totalvotes'], axis = 1)\n",
    "    temp = temp.set_index(['county_fips', 'mode']).astype(int).reset_index()\n",
    "    temp = temp.groupby(['county_fips', 'mode']).sum().reset_index()\n",
    "    total_vote = temp.loc[temp['mode'] == 'TOTAL'].set_index('county_fips')\n",
    "    total_vote = total_vote.drop('mode', axis = 1)\n",
    "    seg_vote = temp.loc[temp['mode'] != 'TOTAL'].groupby('county_fips').sum()\n",
    "    total_vote = pd.concat({'Total': total_vote, 'Alt': seg_vote}, axis = 1)\n",
    "    total_vote = total_vote.max(axis = 1)\n",
    "    total_vote = pd.DataFrame({'repvotes':total_vote})\n",
    "    pop['vote'] = pop['vote'].join(total_vote, on = 'county_fips')\n",
    "    pop['vote'] = pop['vote'].drop_duplicates('county_fips')\n",
    "    pop['vote'] = pop['vote'].drop(['mode', 'candidatevotes'], axis = 1)\n",
    "    del seg_vote, temp, total_vote\n",
    "    \n",
    "    ## calculate percentage voting republican\n",
    "    pop['vote'] = pop['vote'].set_index('county_fips').astype(float)\n",
    "    pop['vote']['reppct'] = pop['vote']['repvotes'] / pop['vote']['totalvotes']\n",
    "    \n",
    "    ## calculate percentage of population that voted\n",
    "    county_total = pd.DataFrame(pop['census']['DP05_0001E'])\n",
    "    county_total['county'] = [i[0:5] for i in county_total.index]\n",
    "    county_total = county_total.groupby('county').sum().astype(int)\n",
    "    pop['vote'] = pop['vote'].join(county_total)\n",
    "    pop['vote']['totalpct'] = pop['vote']['totalvotes'] / pop['vote']['DP05_0001E']\n",
    "    pop['vote'].loc[pop['vote'].totalpct > 1, 'totalpct'] = 159633396 / 331449281\n",
    "    pop['vote'] = pop['vote'][['reppct', 'totalpct']]\n",
    "    \n",
    "    ## merge voting data into census and convert to counts\n",
    "    pop['census']['county'] = [i[0:5] for i in pop['census'].index]\n",
    "    pop['census'] = pop['census'].reset_index().set_index('county').join(pop['vote'])\n",
    "    pop = pop['census'].set_index('GEO_ID')\n",
    "    pop['state'] = [i[0:2] for i in pop.index]\n",
    "    \n",
    "    ## -- impute missing data\n",
    "    \n",
    "    ## impute at the state level\n",
    "    state_mean = pop.copy()[['life_expect', 'reppct', 'totalpct', 'state']]\n",
    "    state_mean = state_mean.groupby('state').mean().round(2)\n",
    "    state_mean = state_mean.loc[pop.state]\n",
    "    for i in state_mean.columns:\n",
    "        j = pop[i].isna().values\n",
    "        pop.loc[j, i] = state_mean.loc[j, i].values\n",
    "    \n",
    "    ## impute at the national level\n",
    "    for i in state_mean.columns:\n",
    "        j = pop[i].isna().values\n",
    "        pop.loc[j, i] = pop[i].mean()\n",
    "    del state_mean\n",
    "    \n",
    "    ## convert vote proportions to counts\n",
    "    pop['rep_vote'] = pop['reppct'] * pop['totalpct'] * pop['DP05_0001E']\n",
    "    pop['rep_vote'] = pop['rep_vote'].round().astype(int)\n",
    "    pop['total_vote'] = (pop['totalpct'] * pop['DP05_0001E']).round().astype(int)\n",
    "    pop = pop.drop(['reppct', 'totalpct', 'state'], axis = 1).round(1)\n",
    "    \n",
    "    ## add prefix to GEO_ID\n",
    "    pop.index = 'ct' + pop.index.astype(str).str.zfill(11)\n",
    "    \n",
    "    ## export result\n",
    "    return pop\n",
    "\n",
    "def read_refine_pop_data():\n",
    "    pop_data = read_pop_data()\n",
    "    pop_data = refine_pop_data(pop = pop_data.copy())\n",
    "    return pop_data\n",
    "    \n",
    "\n",
    "## execute code\n",
    "pop_data = build_or_cache(function = read_refine_pop_data,\n",
    "    address = 'B_Intermediate/pop_data.csv.gz',\n",
    "    permit = set_acceleration['pop_cache'])\n",
    "log_time('RD2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc48382-fdbf-47c5-b7d1-0cbf51cc5691",
   "metadata": {},
   "source": [
    "#### GD3 / RD3 - read and refine text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1a2da6a-4fc5-472a-b366-373019afd73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_explanatory_text(addr = 'A_Input/explanation.docx', n = 47):\n",
    "    explain = Document(addr).paragraphs\n",
    "    explain = [txt_wrap(i.text, n) for i in explain]\n",
    "    explain = '\\n'.join(explain)\n",
    "    return explain\n",
    "\n",
    "## execute code\n",
    "explanatory_text = read_explanatory_text()\n",
    "log_time('RD3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15edf0c1-3995-4c01-9364-0630edb65e14",
   "metadata": {},
   "source": [
    "#### RD4 - Reconcile geographic and population dataset tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9390a35a-4e92-4f56-90c3-adae3c2952f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcile_data(geo = geo_data, pop = pop_data):\n",
    "    pop = pop.loc[geo.index]\n",
    "    return geo, pop\n",
    "\n",
    "##  execute code\n",
    "geo_data, pop_data = reconcile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df65672-a010-40b2-94ac-3906eae80072",
   "metadata": {},
   "source": [
    "# Model Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90542624-5a86-4e97-b5a6-1d3c4845dd75",
   "metadata": {},
   "source": [
    "#### MD1 - Precalculate tract-to-tract distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6fb3cca-b253-4240-8d2f-15f99fa62a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build/Cache Decision: Build\n",
      "Starting 6 engines with <class 'ipyparallel.cluster.launcher.LocalEngineSetLauncher'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097313ae71104a55b7109e5eb1368084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?engine/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58233584ed54c53b02e14853f494541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "measure_distance_parallel_slice:   0%|          | 0/4499 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping engine(s): 1652372201\n",
      "engine set stopped 1652372201: {'engines': {'0': {'exit_code': 0, 'pid': 16773, 'identifier': '0'}, '1': {'exit_code': 0, 'pid': 16774, 'identifier': '1'}, '2': {'exit_code': 0, 'pid': 16775, 'identifier': '2'}, '3': {'exit_code': 0, 'pid': 16776, 'identifier': '3'}, '4': {'exit_code': 0, 'pid': 16777, 'identifier': '4'}, '5': {'exit_code': 0, 'pid': 16778, 'identifier': '5'}}, 'exit_code': 0}\n",
      "Stopping controller\n",
      "Controller stopped: {'exit_code': 0, 'pid': 16761, 'identifier': 'ipcontroller-1652372200-ip8i-16756'}\n"
     ]
    }
   ],
   "source": [
    "## define function to do distance compuations in parallel\n",
    "def measure_distance_in_parallel(geo = geo_data):\n",
    "    \n",
    "    xy = list(zip(geo.INTPTLAT.values, geo.INTPTLON.values))\n",
    "    the_iter = list(range(0, len(xy)))\n",
    "    \n",
    "    ## define engine function that will run on each parallel process\n",
    "    def measure_distance_parallel_slice(n, xy_col = xy):\n",
    "        from geopy.distance import geodesic # for parallel process\n",
    "        xy_col = xy_col.copy()\n",
    "        xy_row = xy_col[n]\n",
    "        xy_dist = []\n",
    "        for i in xy_col[0:n]: xy_dist.append(0)\n",
    "        for i in xy_col[n::]:\n",
    "            xy_dist.append(int(round(geodesic(xy_row, i).miles)))\n",
    "        return xy_dist\n",
    "\n",
    "    ## run engine in parallel for each slice of the data\n",
    "    with Cluster(n = 6) as clust:\n",
    "        view = clust.load_balanced_view()\n",
    "        asyncresult = view.map_async(measure_distance_parallel_slice, the_iter)\n",
    "        asyncresult.wait_interactive()\n",
    "        result = asyncresult.get()\n",
    "        \n",
    "    ## package results and export\n",
    "    result = np.array(result)\n",
    "    result = result + result.T\n",
    "    result = pd.DataFrame(result)\n",
    "    result.index, result.columns = (geo.index, geo.index)\n",
    "    return result\n",
    "\n",
    "## execute code\n",
    "tract_distance = build_or_cache(\n",
    "    function = measure_distance_in_parallel,\n",
    "    address = 'B_Intermediate/tract_distance.csv.gz',\n",
    "    permit = set_acceleration['distance_cache'])\n",
    "log_time('MD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc19f1f2-f863-4872-8427-023b5a662713",
   "metadata": {},
   "source": [
    "#### MD2 - Make one-stage agglomeration clustering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "698a3688-72ee-4f26-8487-ec47ad835e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "## score model\n",
    "def agglom_score(clusters, dist):\n",
    "    \n",
    "    ## load libraries (enables parallel processing)\n",
    "    import numpy as np\n",
    "    \n",
    "    ## construct matrix of points that are in the same group\n",
    "    score = clusters.reshape(clusters.shape[0], 1)\n",
    "    score = (score == score.T).astype(int)\n",
    "    \n",
    "    ## sum distance between points in the same cluster\n",
    "    score = (dist * score).sum().sum()\n",
    "    return score.astype(int)\n",
    "\n",
    "## generate partition of census tracts into k clusters based on proximity\n",
    "def agglom_ml(k, dist, score_func = agglom_score):\n",
    "    \n",
    "    ## load libraries (enables parallel processing)\n",
    "    from sklearn.cluster import AgglomerativeClustering # for parallel process\n",
    "    from numpy           import append\n",
    "    \n",
    "    ## divide census tracts into clusters\n",
    "    ml = AgglomerativeClustering(n_clusters = k,\n",
    "                                 affinity = 'precomputed',\n",
    "                                 linkage = 'average',\n",
    "                                 compute_full_tree = False)\n",
    "    ml_clusters = ml.fit_predict(dist)\n",
    "    \n",
    "    ## score the quality of the cluster solution\n",
    "    ml_score = score_func(ml_clusters, dist)\n",
    "    ml_clusters = append(ml_clusters, ml_score)\n",
    "    \n",
    "    ## export results\n",
    "    return ml_clusters\n",
    "\n",
    "## find best solution (using the fit curve 'elbow' approach)\n",
    "def find_best_cluster_solution(cluster_batch):\n",
    "\n",
    "    ## extract x and y\n",
    "    x = cluster_batch.columns.values\n",
    "    y = cluster_batch.iloc[cluster_batch.shape[0] - 1, :].values\n",
    "    \n",
    "    ## regularize x and y\n",
    "    x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "    y = (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "    x = x.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    ## generate reference x and y\n",
    "    x_ref = np.arange(0, 1, 0.01)\n",
    "    x_ref = x_ref.reshape(-1, 1)\n",
    "    y_ref = 1 - x_ref\n",
    "    x_ref = x_ref\n",
    "    \n",
    "    ## calculate distances between xy and xy_ref\n",
    "    x_dist = (x - x_ref.T)**2\n",
    "    y_dist = (y - y_ref.T)**2\n",
    "    elbow_score = x_dist + y_dist\n",
    "    del x_dist, y_dist, x, y, x_ref, y_ref\n",
    "    \n",
    "    ## find the solution that is furthest from the line of equality (the elbow)\n",
    "    elbow_score = elbow_score.min(axis = 1)\n",
    "    elbow_score = (elbow_score == np.max(elbow_score)).astype(int)\n",
    "    \n",
    "    ## package and export\n",
    "    elbow_score = pd.DataFrame({'Elbow':elbow_score}).T\n",
    "    elbow_score.columns = cluster_batch.columns\n",
    "    cluster_batch = pd.concat([cluster_batch, elbow_score], axis = 0)\n",
    "    \n",
    "    ##  return object\n",
    "    return cluster_batch\n",
    "    \n",
    "## fit clusters in parallel for k = 2 through 50\n",
    "def agglom_batch(k_min = 2, k_max = 48, f = agglom_ml,\n",
    "                 dist_mat = tract_distance,\n",
    "                 cores = set_acceleration['cluster_parallel_cores']):\n",
    "    \n",
    "    ## bound check parameters and construct model list\n",
    "    k_min = max(k_min, 2)\n",
    "    k_max = max(k_min + 1, k_max) + 1\n",
    "    k_range = list(range(k_min, k_max))\n",
    "    \n",
    "    ## set distance matrix default\n",
    "    from functools import partial\n",
    "    f = partial(f, dist = dist_mat)\n",
    "    \n",
    "    ## run models in parallel\n",
    "    with Cluster(n = cores) as clust:\n",
    "        view = clust.load_balanced_view()\n",
    "        asyncresult = view.map_async(f, k_range)\n",
    "        asyncresult.wait_interactive()\n",
    "        result = asyncresult.get()\n",
    "    \n",
    "    ## package results and identity best solution\n",
    "    result = pd.DataFrame(np.array(result).T)\n",
    "    result.columns = k_range\n",
    "    result.index = np.append(dist_mat.index, 'Fit')\n",
    "    result = find_best_cluster_solution(result)\n",
    "    result = pd.DataFrame(result, columns = k_range)\n",
    "    return result\n",
    "\n",
    "## test code\n",
    "cluster_level_one = build_or_cache(\n",
    "    function = agglom_batch,\n",
    "    address = 'B_Intermediate/cluster_level_one.csv.gz',\n",
    "    permit = set_acceleration['cluster_L1_cache'])\n",
    "log_time('MD2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceac255-b803-4fd3-a016-63a5473ef61b",
   "metadata": {},
   "source": [
    "#### MD3 - Conduct two-stage agglomeration clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20f5eba1-fa93-4217-9df6-626a48df66d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "def extract_cluster_solution(cl1):\n",
    "    i = cl1.loc['Elbow', ].astype(bool)\n",
    "    cl1 = cl1.loc[ ~cl1.index.isin(['Fit', 'Elbow']), i].squeeze()\n",
    "    return cl1\n",
    "\n",
    "def agglom_l2(dist = tract_distance, func = agglom_batch,\n",
    "              cl1 = cluster_level_one, ecs = extract_cluster_solution):\n",
    "    \n",
    "    ## extract level one partition and set max number of level two clusters\n",
    "    cl1 = ecs(cl1)\n",
    "    max_cl2_clusters = int(48 / max(cl1)) * 2\n",
    "    \n",
    "    ## generate level two clusters for the cities in each level one cluster\n",
    "    cl2 = dict()\n",
    "    for i in set(cl1):\n",
    "        \n",
    "        ## calculate 2nd level cluster solutions for each cluster\n",
    "        cluster_tracts = cl1[cl1 == i].index\n",
    "        dist_iter = dist.loc[cluster_tracts, cluster_tracts].copy()\n",
    "        k_max_iter = min(max_cl2_clusters, dist.shape[0])\n",
    "        cl2[i] = func(dist_mat = dist_iter, k_max = k_max_iter)\n",
    "        \n",
    "        ## define useful indexes\n",
    "        only_cluster = ~cl2[i].index.isin(['Fit', 'Elbow'])\n",
    "        best_elbow = cl2[i].loc['Elbow'] == 1\n",
    "        \n",
    "        ## extract elbow statistics\n",
    "        #elbow_range = best_elbow.copy()\n",
    "        #elbow_range.iat[0] = True\n",
    "        #elbow_range.iat[elbow_range.shape[0]-1] = True\n",
    "        stats = [cl2[i].loc[~only_cluster, ]]\n",
    "        stat_container = [np.nan for k in range(0, cl2[i].shape[0]-2)]\n",
    "        stat_container[0] = stats\n",
    "\n",
    "        ## extract the best L2 cluster assignments for each L1 cluster\n",
    "        cl2[i] = cl2[i].loc[only_cluster, best_elbow]\n",
    "        cl2[i].columns = ['l2_cluster']\n",
    "        cl2[i]['l1_cluster'] = int(i)\n",
    "        cl2[i]['l2_stats'] = stat_container\n",
    "        \n",
    "    ## consolidate and export data\n",
    "    cl2 = pd.concat(cl2.values(), axis = 0).reset_index()\n",
    "    cl2 = cl2.rename({'index':'GEO_ID'}, axis = 1).set_index('GEO_ID')\n",
    "    return cl2\n",
    "\n",
    "## execute code (Note - stats object is too complex to reassemble correctly.\n",
    "   ## This ok for now as it is only being saved for 'just-in-case' reasons.\n",
    "   ## If needed, it can be converted to a csv-format text string\n",
    "cluster_level_two = build_or_cache(\n",
    "    function = agglom_l2,\n",
    "    address  = 'B_Intermediate/cluster_level_two.csv.gz',\n",
    "    permit   = set_acceleration['cluster_L2_cache']\n",
    "    )\n",
    "geo_data = geo_data.join(cluster_level_two)\n",
    "del cluster_level_two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665fce9-de4f-4319-923f-e351217c78a3",
   "metadata": {},
   "source": [
    "# Enrich Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d58595a4-ffa6-407a-9fce-a86f9bfadef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time log:\n",
      "   H2: 12:16:30\n",
      "   H3: 12:16:30\n",
      "  GD1: 12:16:33\n",
      "  RD1: 12:16:33\n",
      "  RD2: 12:16:40\n",
      "  RD3: 12:16:40\n",
      "  MD1: 12:23:25\n",
      "  MD2: 12:23:45\n",
      "  End: 12:25:28\n"
     ]
    }
   ],
   "source": [
    "log_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51179468-2a82-4db1-be14-74e83d629377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b6438-6ef8-4ea2-b4d8-a5039053e607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
